{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c454805-31c0-4e82-bb47-2a0a21dd565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8cc4ec-beaa-49d5-a421-63aa510f4970",
   "metadata": {},
   "source": [
    "# Download BeanCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c4b05-4a03-4179-a126-8632ff363225",
   "metadata": {},
   "source": [
    "Analysis in paper based on Train + Validation \\\n",
    "Change <\"YOUR_HUGGINGFACE_USER\"> to your own huggingface username; account can be made at https://huggingface.co/join \\\n",
    "Only run the following command to download if you have sufficient storage\\\n",
    "-- train split: 142 GB \\\n",
    "-- val split: 25MB \\\n",
    "-- sample split: 1.5GB \\\n",
    "-- deduped split: 60GB \\\n",
    "-- fraud split: 411MB \\\n",
    "You can choose to download a specific split by using --include \"split_name/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d1ba9-2393-46f1-a91f-03c96015e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!GIT_LFS_SKIP_SMUDGE=1 git clone https://<YOUR_HUGGINGFACE_USER>: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@huggingface.co/datasets/blevy41/BeanCounter\n",
    "!cd BeanCounter\n",
    "!git lfs pull --include \"train/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2281d-9550-4b0a-b59c-4716500f13f4",
   "metadata": {},
   "source": [
    "# Set up directories & pronoun dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9d7e5af-4a33-473a-8600-75c1232a4fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = ''\n",
    "BEANCOUNTER_DATASET_PATH = 'beancounter'\n",
    "TRAIN_SPLIT = 'train'\n",
    "VAL_SPLIT = 'validation'\n",
    "SAMPLE_PATH = 'sample'\n",
    "RESULTS_DIR = '' # specify dir for saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8115a03e-c559-4f6e-aaea-a8fd19468920",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadpronoun2variation = {\n",
    "    'She': ['she', 'her', 'hers', 'herself'], \n",
    "    'He': ['he', 'him', 'his', 'himself'], \n",
    "    'Unknown': ['they', 'them', 'their', 'theirs', 'theirself', 'themself', 'themselves'],\n",
    "    '1st_person': ['I', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'], \n",
    "    '2nd_person': ['you', 'your', 'yours', 'yourself', 'yourselves'],\n",
    "    '3rd_person': ['it', 'its', 'itself', 'she', 'her', 'hers', 'herself', 'he', 'him', 'his', 'himself', 'they', 'them', 'their', 'theirs', 'theirself', 'themself', 'themselves'] \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182be9dc-8431-4617-b19d-4f71b7a61992",
   "metadata": {},
   "source": [
    "# define text normalization and getting pronouns count of each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d09ea339-e256-450a-891a-1603c1d5d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_OR_MORE_SPACE = re.compile(r\"\\s+\")\n",
    "ALL_PUNCTUATION = re.compile(r'''(\\w+(?:[.,-]\\w+)*)|[!\\\"#\\$%&\\'\\(\\)\\*\\+,-\\.\\/:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~]+''')\n",
    "PUNC_SURROUNDED_BY_SPACE = re.compile(r'''(?<!\\S)[!\\\"#\\$%&\\'\\(\\)\\*\\+,-\\.\\/:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~]+(?!\\S)''')\n",
    "\n",
    "def text_normalization(text):\n",
    "    '''normalizations for removing nonbreaking spaces, white spaces, and punctuation'''\n",
    "    \n",
    "    normalized_str = unicodedata.normalize('NFKD', text) # remove nonbreaking spaces\n",
    "    normalized_str = re.sub(ONE_OR_MORE_SPACE, ' ', normalized_str)\n",
    "    normalized_str = re.sub(ALL_PUNCTUATION, r'\\1', normalized_str)\n",
    "    normalized_str = re.sub(PUNC_SURROUNDED_BY_SPACE, '', normalized_str)\n",
    "    return normalized_str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11f18681-b5e4-4ed0-ace6-44574ccd1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pronouns = list(broadpronoun2variation.values())\n",
    "all_pronouns = [j for i in all_pronouns for j in i]\n",
    "all_pronouns = set(all_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "613a1290-132a-4040-9f04-de03c5200a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronouns_count(cleaned_text, all_pronouns = all_pronouns):\n",
    "    \"\"\"\n",
    "    check what kind of pronouns a single filing has\n",
    "    parameters: cleaned_text - str, all_pronouns - list of strs\n",
    "    return: pronoun categories with the correct flags (0 or 1, 0 means no pronouns of this category/type in the attachment and 1 means at least 1 instance)\n",
    "    \"\"\"\n",
    "    pronouns2count = {p : 0 for p in all_pronouns}\n",
    "    broad_pronoun2flag = {'She': 0, 'He': 0, 'Unknown': 0,'1st_person': 0, '2nd_person': 0, '3rd_person': 0}\n",
    "    type2count = {'grammatical': 0, 'gender': 0}\n",
    "    words = text_normalization(cleaned_text).split()\n",
    "    pronouns_in_this_filing = set()\n",
    "    for word in words:\n",
    "        if word.strip().lower() in all_pronouns:\n",
    "            pronouns2count[word] += 1\n",
    "    \n",
    "    for pronoun, count in pronouns2count.items():\n",
    "        if count > 0:\n",
    "            for category in broad_pronoun2flag:\n",
    "                if pronoun in broadpronoun2variation[category] and broad_pronoun2flag[category] == 0:\n",
    "                    broad_pronoun2flag[category] = 1\n",
    "    \n",
    "    if broad_pronoun2flag['She'] == 1 or broad_pronoun2flag['He'] == 1 or broad_pronoun2flag['Unknown'] == 1:\n",
    "        type2count['gender'] = 1\n",
    "    if  broad_pronoun2flag['1st_person'] == 1 or broad_pronoun2flag['2nd_person'] == 1 or broad_pronoun2flag['3rd_person'] == 1:\n",
    "        type2count['grammatical'] = 1\n",
    "\n",
    "    return type2count, broad_pronoun2flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced2818-63da-41e5-bf21-0e6ebc6c9f18",
   "metadata": {},
   "source": [
    "# Get pronouns count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65d510a5-52c4-4876-bccb-59a0b5bbd555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition of dataset to perform pronoun analysis on. \n",
    "# If want to analyze whole dataset, include both TRAIN_SPLIT and VAL_SPLIT. If just sample, include SAMPLE_PATH\n",
    "\n",
    "def get_files_to_process(train_path, validation_path):\n",
    "    files_to_process = []\n",
    "    # load files from train split\n",
    "    for path, dirs, fns in os.walk(train_path):\n",
    "        for fn in fns:\n",
    "            if os.path.splitext(fn)[-1] == '.gz':\n",
    "                files_to_process.append(os.path.join(path, fn))\n",
    "    \n",
    "    # load files from validation split\n",
    "    for path, dirs, fns in os.walk(validation_path):\n",
    "        for fn in fns:\n",
    "            if os.path.splitext(fn)[-1] == '.gz':\n",
    "                files_to_process.append(os.path.join(path, fn))\n",
    "    return files_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99009d-77c3-4700-8923-5a22f3a654e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def work(path, out_dir = os.path.join(ROOT_DIR, RESULTS_DIR)):\n",
    "    force = False\n",
    "    path_out = os.path.join(out_dir, path.split('/')[4])\n",
    "    fn = path.rsplit('/', 1)[1]\n",
    "    \n",
    "    if os.path.exists(os.path.join(path_out, fn)) and not force:\n",
    "        return False\n",
    "\n",
    "    os.makedirs(path_out, exist_ok = True)\n",
    "    \n",
    "    # write .lock file\n",
    "    lock_file = os.path.join(path_out, fn.rsplit('.', 1)[0])+'.lock'\n",
    "    open(lock_file, 'w').close()\n",
    "    \n",
    "    with gzip.open(path, 'r') as filings:\n",
    "        all_filings = []\n",
    "        for line in filings.readlines():\n",
    "            desc_dict = {}\n",
    "            info = json.loads(line)\n",
    "            desc_dict['accession'] = info['accession']\n",
    "            desc_dict['filename'] = info['filename']\n",
    "            desc_dict['type_attachment'] = info['type_attachment']\n",
    "            desc_dict['ts_accept'] = info['ts_accept']\n",
    "            desc_dict['form_type'] = info['form_type']\n",
    "            desc_dict['type_filing'] = info['type_filing']\n",
    "            desc_dict['date'] = info['ts_accept'].split('T')[0]\n",
    "            pronouns_info = get_pronouns_count(info['cleaned_text'])\n",
    "            broad_pronouns = pronouns_info[0]\n",
    "            specific_pronouns = pronouns_info[1]\n",
    "            desc_dict['grammatical'] = broad_pronouns['grammatical']\n",
    "            desc_dict['gender'] = broad_pronouns['gender']\n",
    "            desc_dict['She'] = specific_pronouns['She']\n",
    "            desc_dict['He'] = specific_pronouns['He']\n",
    "            desc_dict['Unknown'] = specific_pronouns['Unknown']\n",
    "            desc_dict['1st_person'] = specific_pronouns['1st_person']\n",
    "            desc_dict['2nd_person'] = specific_pronouns['2nd_person']\n",
    "            desc_dict['3rd_person'] = specific_pronouns['3rd_person']\n",
    "            \n",
    "            all_filings.append(desc_dict)\n",
    "        \n",
    "        with gzip.open(os.path.join(path_out, fn), 'w') as out_file:\n",
    "            for entry in all_filings:\n",
    "                out_file.write((json.dumps(entry) + '\\n').encode())\n",
    "    \n",
    "    os.remove(lock_file)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e052211-5b6d-49f8-b687-e0669f3a6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(ROOT_DIR, BEANCOUNTER_DATASET_PATH, TRAIN_SPLIT)\n",
    "val_path = os.path.join(ROOT_DIR, BEANCOUNTER_DATASET_PATH, VAL_SPLIT)\n",
    "files_to_process = get_files_to_process(train_path, val_path)\n",
    "os.makedirs(os.path.join(ROOT_DIR, RESULTS_DIR), exist_ok = True)\n",
    "\n",
    "n_works = 16 # change to desired number of workers\n",
    "with mp.Pool(n_workers) as p:\n",
    "    results = [r for r in tqdm(p.imap_unordered(work, files_to_process), total=len(files_to_process), miniters=1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
