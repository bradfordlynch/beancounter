{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f13ef-db2b-4385-9d6c-a9cba272a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adaped from https://github.com/vrunm/Text-Classification-Financial-Phrase-Bank/blob/main/1.text-classification-bert-financial-phrase-bank.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6167bb07-2f55-4da0-b370-f8469eaa9d20",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d72e786d-9755-48d0-aef8-8f5ea2c04373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ab005-67fb-49e6-a185-ff4abfe1356c",
   "metadata": {},
   "source": [
    "# helper functions to set up FPB dataset and model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd77ca2-e724-46fe-a8bb-7f69f2e7211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset_df(dataset_path, data_split):\n",
    "    fpb_dataset = load_dataset(dataset_path, data_split)\n",
    "    financial_data = pd.DataFrame(fpb_dataset['train'])\n",
    "    financial_data = financial_data.rename(columns={'sentence':'NewsHeadline'})\n",
    "    num2sentiment = {0: 'negative', 1:'neutral', 2:'positive'}\n",
    "    financial_data['sentiment'] = financial_data['label'].apply(lambda x:num2sentiment[x])\n",
    "    financial_data = financial_data[['NewsHeadline', 'sentiment']]\n",
    "    return financial_data\n",
    "\n",
    "# Label encode the sentiment values\n",
    "# The unique values in sentiment column are returned as a NumPy array.\n",
    "# Enumerate method adds counter to an iterable and returns it. The returned object is an enumerate object.\n",
    "# Convert enumerate objects to list.\n",
    "def encode_sentiments_values(df):\n",
    "    possible_sentiments = df.sentiment.unique()\n",
    "    sentiment_dict = {}\n",
    "\n",
    "    for index, possible_sentiment in enumerate(possible_sentiments):\n",
    "        sentiment_dict[possible_sentiment] = index\n",
    "\n",
    "    # Encode all the sentiment values\n",
    "    df[\"label\"] = df.sentiment.replace(sentiment_dict)\n",
    "\n",
    "    return df, sentiment_dict\n",
    "\n",
    "def get_x_y_val_and_train(financial_data):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        financial_data.index.values,\n",
    "        financial_data.label.values,\n",
    "        test_size=0.20,\n",
    "        random_state=2022,\n",
    "        stratify=financial_data.label.values,\n",
    "    )\n",
    "    \n",
    "    X_val = financial_data.filter(items=X_val, axis=0)\n",
    "    X_train = financial_data.filter(items=X_train, axis=0)\n",
    "    return X_val, X_train, y_train, y_val\n",
    "\n",
    "def init_model_tokenizer(name_model):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name_model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(name_model, num_labels=len(sentiment_dict))\n",
    "    \n",
    "    if model.config.pad_token_id is None: \n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "def get_train_val_datasets(X_val, X_train, y_train, y_val):\n",
    "    # Encode the Training and Validation Data\n",
    "    # encode_plus method performs the following tasks:\n",
    "    # split our news headlines into tokens,\n",
    "    # add the special [CLS] and [SEP] tokens\n",
    "    # convert these tokens into indexes of the tokenizer vocabulary,\n",
    "    # pad or truncate sentences to max length, then finally create an attention mask.\n",
    "    \n",
    "    # return_tensors (str, optional, defaults to None) – Can be set to ‘tf’ or ‘pt’ to return respectively TensorFlow tf.constant or PyTorch torch.Tensor instead of a list of python integers.\n",
    "    \n",
    "    # add_special_tokens (bool, optional, defaults to True) – If set to True, the sequences will be encoded with the special tokens relative to their model.\n",
    "    \n",
    "    # return_attention_masks (bool, optional, defaults to none) –\n",
    "    \n",
    "    # Whether to return the attention mask. If left to the default, will return the attention mask according to the specific tokenizer’s default,\n",
    "    \n",
    "    # pad_to_max_length (bool, optional, defaults to False) –\n",
    "    # If set to True, the returned sequences will be padded according to the model’s padding side and padding index, up to their max length.\n",
    "    \n",
    "    # max_length (int, optional, defaults to None) – If set to a number, will limit the total sequence returned so that it has a maximum length\n",
    "    # 150 is used since it is the maximum length observed in the headlines\n",
    "    \n",
    "    encoded_data_train = tokenizer.batch_encode_plus(\n",
    "        X_train.NewsHeadline.to_list(),\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        max_length=150,\n",
    "        padding = 'max_length'\n",
    "    )\n",
    "    \n",
    "    encoded_data_val = tokenizer.batch_encode_plus(\n",
    "        X_val.NewsHeadline.to_list(),\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        max_length=150,\n",
    "        padding = 'max_length'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    input_ids_train = encoded_data_train[\"input_ids\"]\n",
    "    attention_masks_train = encoded_data_train[\"attention_mask\"]\n",
    "    labels_train = torch.tensor(y_train)\n",
    "    \n",
    "    input_ids_val = encoded_data_val[\"input_ids\"]\n",
    "    attention_masks_val = encoded_data_val[\"attention_mask\"]\n",
    "    sentiments_val = torch.tensor(y_val)\n",
    "    \n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, sentiments_val)\n",
    "    return dataset_train, dataset_val\n",
    "\n",
    "def train(dataset_train, dataset_val, model, out_dir):\n",
    "    ###Torch DataLoader\n",
    "    # torch.utils.data.RandomSampler(data_source, replacement=False, num_samples=None, generator=None)\n",
    "    # Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify num_samples to draw.\n",
    "    # data_source (Dataset) – dataset to sample from\n",
    "    # replacement (bool) – samples are drawn on-demand with replacement if True, default=``False``\n",
    "    # num_samples (int) – number of samples to draw, default=`len(dataset)`\n",
    "    \n",
    "    # torch.utils.data.SequentialSampler(data_source)\n",
    "    # Samples elements sequentially, always in the same order.\n",
    "    # data_source (Dataset) – dataset to sample from\n",
    "    \n",
    "    batch_size = 32\n",
    "    \n",
    "    dataloader_train = DataLoader(\n",
    "        dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    dataloader_validation = DataLoader(\n",
    "        dataset_val, sampler=RandomSampler(dataset_val), batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # To construct an Optimizer you have to give it an iterable containing the parameters (all should be Variable s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.\n",
    "    \n",
    "    # torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False, *, maximize=False, foreach=None, capturable=False)\n",
    "    \n",
    "    # transformers.get_linear_schedule_with_warmup\n",
    "    # Parameters optimizer (~torch.optim.Optimizer) — The optimizer for which to schedule the learning rate.\n",
    "    # num_warmup_steps (int) — The number of steps for the warmup phase.\n",
    "    # num_training_steps (int) — The total number of training steps.\n",
    "    # last_epoch (int, optional, defaults to -1) — The index of the last epoch when resuming training.\n",
    "    epochs = 3\n",
    "    optimizer1 = torch.optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer1, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs\n",
    "    )\n",
    "    \n",
    "    \n",
    "    seed_val = 2022\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        model.train()\n",
    "    \n",
    "        loss_train_total = 0\n",
    "    \n",
    "        progress_bar = tqdm(\n",
    "            dataloader_train, desc=\"Epoch {:1d}\".format(epoch), leave=False, disable=False\n",
    "        )\n",
    "        for batch in progress_bar:\n",
    "            model.zero_grad()\n",
    "    \n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "    \n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"labels\": batch[2],\n",
    "            }\n",
    "    \n",
    "            outputs = model(**inputs)\n",
    "    \n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "    \n",
    "            # Gradient Clipping is done to restrict the values of the gradient(To prevent the model from exploding gradients)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "            optimizer1.step()\n",
    "            scheduler.step()\n",
    "    \n",
    "            progress_bar.set_postfix(\n",
    "                {\"training_loss\": \"{:.3f}\".format(loss.item() / len(batch))}\n",
    "            )\n",
    "            \n",
    "        os.makedirs(out_dir, exist_ok = True)\n",
    "        torch.save(model.state_dict(), f\"{out_dir}/finetuned_{name_model.split('/')[-1]}_epoch_{epoch}.model\")\n",
    "    \n",
    "        tqdm.write(f\"\\nEpoch {epoch}\")\n",
    "    \n",
    "        loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "        tqdm.write(f\"Training loss: {loss_train_avg}\")\n",
    "    \n",
    "        val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "        val_f1 = f1_score(predictions.argmax(axis=1), true_vals, average=\"weighted\")\n",
    "        tqdm.write(f\"Validation loss: {val_loss}\")\n",
    "        tqdm.write(f\"F1 Score (Weighted): {val_f1}\")\n",
    "\n",
    "    return dataloader_train, dataloader_validation\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"labels\": batch[2],\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs[\"labels\"].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "\n",
    "def eval_best_ckpt(model, out_dir, best_epoch_num, dataloader_validation):\n",
    "    model.to(device)\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"{out_dir}/finetuned_{name_model.split('/')[-1]}_epoch_{best_epoch_num}.model\", map_location=torch.device(\"cpu\"))\n",
    "    )\n",
    "    _, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_score(predictions.argmax(axis=1), true_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2398aee-fe90-495c-baea-0b9a5c95b216",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80a153-6bd7-4962-b7c5-424c0ac7b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_data = prep_dataset_df('financial_phrasebank', 'sentences_allagree')\n",
    "financial_data, sentiment_dict = encode_sentiments_values(financial_data)\n",
    "X_val, X_train, y_train, y_val = get_x_y_val_and_train(financial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9111d-226c-499d-a9b4-011930af08f5",
   "metadata": {},
   "source": [
    "# phi base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79393f5-de5d-4c97-a9fd-982b35f07a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'microsoft/phi-1_5'\n",
    "out_dir = './resulting_models/phi'\n",
    "tokenizer, model = init_model_tokenizer(name_model)\n",
    "dataset_train, dataset_val = get_train_val_datasets(X_val, X_train, y_train, y_val)\n",
    "dataloader_train, dataloader_validation = train(dataset_train, dataset_val, model, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2008a-5b4a-46da-b8ca-8ba59c3c6cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_num = 3 #change epoch number to the best performing epoch\n",
    "eval_best_ckpt(model, out_dir, best_epoch_num, dataloader_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52baaa0e-5548-4187-b741-08cdb4437afb",
   "metadata": {},
   "source": [
    "# pythia base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8560761-cb5f-4dda-bb20-8e386375e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'EleutherAI/pythia-1.4b'\n",
    "out_dir = './resulting_models/pythia'\n",
    "tokenizer, model = init_model_tokenizer(name_model)\n",
    "dataset_train, dataset_val = get_train_val_datasets(X_val, X_train, y_train, y_val)\n",
    "dataloader_train, dataloader_validation = train(dataset_train, dataset_val, model, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d3af7-3360-448a-bd1a-6b2b5f59f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_num = 3 #change epoch number to the best performing epoch\n",
    "eval_best_ckpt(model, out_dir, best_epoch_num, dataloader_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e84624-94c5-4d07-ac08-ddaa9c207617",
   "metadata": {},
   "source": [
    "# phi-beancounter model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31408a99",
   "metadata": {},
   "source": [
    "You will need to first go to https://huggingface.co/bradfordlevy/phi-1_5-bc-cp, agree to share contact information, login using huggingface-cli and then access the model. Learn how to generate access token here: https://huggingface.co/docs/hub/en/security-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e9d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c185cc-0751-471c-aa77-e94cb72d6634",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'bradfordlevy/phi-1_5-bc-cp'\n",
    "out_dir = './resulting_models/phi_bc'\n",
    "tokenizer, model = init_model_tokenizer(name_model)\n",
    "dataset_train, dataset_val = get_train_val_datasets(X_val, X_train, y_train, y_val)\n",
    "dataloader_train, dataloader_validation = train(dataset_train, dataset_val, model, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39106326-702f-4731-9b50-0734bdc13611",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_num = 3 #change epoch number to the best performing epoch\n",
    "eval_best_ckpt(model, out_dir, best_epoch_num, dataloader_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfd1b4-2d5c-4dac-a4c2-d47ad275407b",
   "metadata": {},
   "source": [
    "# pythia-beancounter model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f568ef1",
   "metadata": {},
   "source": [
    "You will need to first go to https://huggingface.co/bradfordlevy/pythia-1.4b-bc-cp, agree to share contact information, login using huggingface-cli and then access the model. Learn how to generate access token here: https://huggingface.co/docs/hub/en/security-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddaf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc871c-bee3-4a61-927e-3795c3cdde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'bradfordlevy/pythia-1.4b-bc-cp'\n",
    "out_dir = './resulting_models/pythia_bc'\n",
    "tokenizer, model = init_model_tokenizer(name_model)\n",
    "dataset_train, dataset_val = get_train_val_datasets(X_val, X_train, y_train, y_val)\n",
    "dataloader_train, dataloader_validation = train(dataset_train, dataset_val, model, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca0a42-85e6-44a4-918f-76eb5570d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_num = 3 #change epoch number to the best performing epoch\n",
    "eval_best_ckpt(model, out_dir, best_epoch_num, dataloader_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
