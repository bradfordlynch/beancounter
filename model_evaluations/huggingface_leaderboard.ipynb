{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84fd928-5084-420a-b80e-62a04113ea1b",
   "metadata": {},
   "source": [
    "# Set up lm_evaluation_harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750d1d3-52ed-4620-a0c0-56d2389d4766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://github.com/EleutherAI/lm-evaluation-harness for details and other tasks for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9273ab-2478-467e-a38b-e80f3dee42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
    "!cd lm-evaluation-harness\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c8079ee-d1d5-400d-a19f-f0527dfa3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import api\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a15b7d",
   "metadata": {},
   "source": [
    "# Get access to huggingface models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6571e318",
   "metadata": {},
   "source": [
    "To access the phi and pythia models that have been continued-pretrained on BeanCounter, do the following steps:\n",
    "\n",
    "To access the continued-pretrained phi model, go to https://huggingface.co/bradfordlevy/phi-1_5-bc-cp, agree to share contact information, login using huggingface-cli and then access the model. Learn how to generate access token here: https://huggingface.co/docs/hub/en/security-tokens\n",
    "\n",
    "Additionally, please follow the same above steps if you want to access the continued-pretrained pythia model here: https://huggingface.co/bradfordlevy/pythia-1.4b-bc-cp.\n",
    "\n",
    "Afterwards, make sure you are logged in via the huggingface cli with the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e304c7cb-f271-431b-aa7d-707b3c8a051a",
   "metadata": {},
   "source": [
    "# Big bench hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e9eb971-b817-454c-a128-cf507814ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_leaderboard_bbh_string = '''\n",
    "group: leaderboard_bbh\n",
    "task:\n",
    "  - leaderboard_bbh_boolean_expressions\n",
    "  - leaderboard_bbh_causal_judgement\n",
    "  - leaderboard_bbh_date_understanding\n",
    "  - leaderboard_bbh_disambiguation_qa\n",
    "  - leaderboard_bbh_formal_fallacies\n",
    "  - leaderboard_bbh_geometric_shapes\n",
    "  - leaderboard_bbh_hyperbaton\n",
    "  - leaderboard_bbh_logical_deduction_five_objects\n",
    "  - leaderboard_bbh_logical_deduction_seven_objects\n",
    "  - leaderboard_bbh_logical_deduction_three_objects\n",
    "  - leaderboard_bbh_movie_recommendation\n",
    "  - leaderboard_bbh_navigate\n",
    "  - leaderboard_bbh_object_counting\n",
    "  - leaderboard_bbh_penguins_in_a_table\n",
    "  - leaderboard_bbh_reasoning_about_colored_objects\n",
    "  - leaderboard_bbh_ruin_names\n",
    "  - leaderboard_bbh_salient_translation_error_detection\n",
    "  - leaderboard_bbh_snarks\n",
    "  - leaderboard_bbh_sports_understanding\n",
    "  - leaderboard_bbh_temporal_sequences\n",
    "  - leaderboard_bbh_tracking_shuffled_objects_five_objects\n",
    "  - leaderboard_bbh_tracking_shuffled_objects_seven_objects\n",
    "  - leaderboard_bbh_tracking_shuffled_objects_three_objects\n",
    "  - leaderboard_bbh_web_of_lies\n",
    "'''\n",
    "\n",
    "with open('./yaml_configs/leaderboard_bbh.yaml', 'w') as f:\n",
    "    f.write(YAML_leaderboard_bbh_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a892d9-9ed7-4d6c-bf6c-0d927431e7ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.01it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1078.49it/s]\n",
      "100%|███████████████████████████████████████| 187/187 [00:00<00:00, 1075.92it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1066.04it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1074.12it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1075.04it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1056.76it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1069.63it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1046.19it/s]\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 690.80it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1051.83it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1068.06it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1071.61it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1016.55it/s]\n",
      "100%|███████████████████████████████████████| 146/146 [00:00<00:00, 1069.46it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1043.15it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1057.11it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1048.28it/s]\n",
      "100%|███████████████████████████████████████| 178/178 [00:00<00:00, 1049.84it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1065.58it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1058.59it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1056.48it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1051.49it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1059.41it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1071.17it/s]\n",
      "Checking cached requests: 100%|████████| 31710/31710 [00:01<00:00, 22565.29it/s]\n",
      "Running loglikelihood requests:   0%|                 | 0/31710 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|█████| 31710/31710 [18:43<00:00, 28.22it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/mnt/spinning/beancounter_replication/continually_pretrained_models/phi-1_5-bc-cp-highlr-hf), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|                          Tasks                           |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|----------------------------------------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_bbh                                           |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_bbh_boolean_expressions                    |      1|none  |     3|acc_norm|↑  |0.5040|±  |0.0317|\n",
      "| - leaderboard_bbh_causal_judgement                       |      1|none  |     3|acc_norm|↑  |0.5294|±  |0.0366|\n",
      "| - leaderboard_bbh_date_understanding                     |      1|none  |     3|acc_norm|↑  |0.1800|±  |0.0243|\n",
      "| - leaderboard_bbh_disambiguation_qa                      |      1|none  |     3|acc_norm|↑  |0.3280|±  |0.0298|\n",
      "| - leaderboard_bbh_formal_fallacies                       |      1|none  |     3|acc_norm|↑  |0.5280|±  |0.0316|\n",
      "| - leaderboard_bbh_geometric_shapes                       |      1|none  |     3|acc_norm|↑  |0.0880|±  |0.0180|\n",
      "| - leaderboard_bbh_hyperbaton                             |      1|none  |     3|acc_norm|↑  |0.5120|±  |0.0317|\n",
      "| - leaderboard_bbh_logical_deduction_five_objects         |      1|none  |     3|acc_norm|↑  |0.1880|±  |0.0248|\n",
      "| - leaderboard_bbh_logical_deduction_seven_objects        |      1|none  |     3|acc_norm|↑  |0.1520|±  |0.0228|\n",
      "| - leaderboard_bbh_logical_deduction_three_objects        |      1|none  |     3|acc_norm|↑  |0.3440|±  |0.0301|\n",
      "| - leaderboard_bbh_movie_recommendation                   |      1|none  |     3|acc_norm|↑  |0.2480|±  |0.0274|\n",
      "| - leaderboard_bbh_navigate                               |      1|none  |     3|acc_norm|↑  |0.5480|±  |0.0315|\n",
      "| - leaderboard_bbh_object_counting                        |      1|none  |     3|acc_norm|↑  |0.1360|±  |0.0217|\n",
      "| - leaderboard_bbh_penguins_in_a_table                    |      1|none  |     3|acc_norm|↑  |0.2123|±  |0.0340|\n",
      "| - leaderboard_bbh_reasoning_about_colored_objects        |      1|none  |     3|acc_norm|↑  |0.1120|±  |0.0200|\n",
      "| - leaderboard_bbh_ruin_names                             |      1|none  |     3|acc_norm|↑  |0.2080|±  |0.0257|\n",
      "| - leaderboard_bbh_salient_translation_error_detection    |      1|none  |     3|acc_norm|↑  |0.2240|±  |0.0264|\n",
      "| - leaderboard_bbh_snarks                                 |      1|none  |     3|acc_norm|↑  |0.5393|±  |0.0375|\n",
      "| - leaderboard_bbh_sports_understanding                   |      1|none  |     3|acc_norm|↑  |0.4600|±  |0.0316|\n",
      "| - leaderboard_bbh_temporal_sequences                     |      1|none  |     3|acc_norm|↑  |0.2840|±  |0.0286|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_five_objects |      1|none  |     3|acc_norm|↑  |0.1800|±  |0.0243|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_seven_objects|      1|none  |     3|acc_norm|↑  |0.1560|±  |0.0230|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_three_objects|      1|none  |     3|acc_norm|↑  |0.3320|±  |0.0298|\n",
      "| - leaderboard_bbh_web_of_lies                            |      1|none  |     3|acc_norm|↑  |0.5040|±  |0.0317|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bradfordlevy/phi-1_5-bc-cp \\\n",
    "    --include_path ./yaml_configs/leaderboard_bbh.yaml \\\n",
    "    --tasks leaderboard_bbh \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_bbh/phi1_5_bc_cp/ \\\n",
    "    --output_path ./resulting_scores/learderboard_bbh/phi1_5_bc_cp/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7496a928-959f-4cea-9ba4-a682f430a69f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/biz/Documents/research/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 944.90it/s]\n",
      "100%|████████████████████████████████████████| 187/187 [00:00<00:00, 937.03it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1005.75it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1046.95it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1046.32it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1031.74it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1048.06it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1024.17it/s]\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 685.19it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1038.58it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1036.41it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1050.92it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1004.77it/s]\n",
      "100%|███████████████████████████████████████| 146/146 [00:00<00:00, 1040.42it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1021.66it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1036.11it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1019.01it/s]\n",
      "100%|███████████████████████████████████████| 178/178 [00:00<00:00, 1021.45it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1041.01it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1030.32it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1029.97it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1023.19it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1028.47it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1044.04it/s]\n",
      "Checking cached requests: 100%|████████| 31710/31710 [00:02<00:00, 13783.63it/s]\n",
      "Running loglikelihood requests:   0%|                 | 0/31710 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 16\n",
      "Running loglikelihood requests: 100%|█████| 31710/31710 [06:28<00:00, 81.54it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=microsoft/phi-1_5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (16)\n",
      "|                          Tasks                           |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|----------------------------------------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_bbh                                           |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_bbh_boolean_expressions                    |      1|none  |     3|acc_norm|↑  |0.6400|±  |0.0304|\n",
      "| - leaderboard_bbh_causal_judgement                       |      1|none  |     3|acc_norm|↑  |0.5348|±  |0.0366|\n",
      "| - leaderboard_bbh_date_understanding                     |      1|none  |     3|acc_norm|↑  |0.2000|±  |0.0253|\n",
      "| - leaderboard_bbh_disambiguation_qa                      |      1|none  |     3|acc_norm|↑  |0.4800|±  |0.0317|\n",
      "| - leaderboard_bbh_formal_fallacies                       |      1|none  |     3|acc_norm|↑  |0.5320|±  |0.0316|\n",
      "| - leaderboard_bbh_geometric_shapes                       |      1|none  |     3|acc_norm|↑  |0.2920|±  |0.0288|\n",
      "| - leaderboard_bbh_hyperbaton                             |      1|none  |     3|acc_norm|↑  |0.4840|±  |0.0317|\n",
      "| - leaderboard_bbh_logical_deduction_five_objects         |      1|none  |     3|acc_norm|↑  |0.2240|±  |0.0264|\n",
      "| - leaderboard_bbh_logical_deduction_seven_objects        |      1|none  |     3|acc_norm|↑  |0.1600|±  |0.0232|\n",
      "| - leaderboard_bbh_logical_deduction_three_objects        |      1|none  |     3|acc_norm|↑  |0.3560|±  |0.0303|\n",
      "| - leaderboard_bbh_movie_recommendation                   |      1|none  |     3|acc_norm|↑  |0.4000|±  |0.0310|\n",
      "| - leaderboard_bbh_navigate                               |      1|none  |     3|acc_norm|↑  |0.4200|±  |0.0313|\n",
      "| - leaderboard_bbh_object_counting                        |      1|none  |     3|acc_norm|↑  |0.2160|±  |0.0261|\n",
      "| - leaderboard_bbh_penguins_in_a_table                    |      1|none  |     3|acc_norm|↑  |0.2466|±  |0.0358|\n",
      "| - leaderboard_bbh_reasoning_about_colored_objects        |      1|none  |     3|acc_norm|↑  |0.1760|±  |0.0241|\n",
      "| - leaderboard_bbh_ruin_names                             |      1|none  |     3|acc_norm|↑  |0.0880|±  |0.0180|\n",
      "| - leaderboard_bbh_salient_translation_error_detection    |      1|none  |     3|acc_norm|↑  |0.2240|±  |0.0264|\n",
      "| - leaderboard_bbh_snarks                                 |      1|none  |     3|acc_norm|↑  |0.5000|±  |0.0376|\n",
      "| - leaderboard_bbh_sports_understanding                   |      1|none  |     3|acc_norm|↑  |0.4760|±  |0.0316|\n",
      "| - leaderboard_bbh_temporal_sequences                     |      1|none  |     3|acc_norm|↑  |0.1960|±  |0.0252|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_five_objects |      1|none  |     3|acc_norm|↑  |0.2240|±  |0.0264|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_seven_objects|      1|none  |     3|acc_norm|↑  |0.1160|±  |0.0203|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_three_objects|      1|none  |     3|acc_norm|↑  |0.3280|±  |0.0298|\n",
      "| - leaderboard_bbh_web_of_lies                            |      1|none  |     3|acc_norm|↑  |0.5160|±  |0.0317|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=microsoft/phi-1_5 \\\n",
    "    --include_path ./yaml_configs/leaderboard_bbh.yaml \\\n",
    "    --tasks leaderboard_bbh \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_bbh/phi1_5/ \\\n",
    "    --output_path ./resulting_scores/learderboard_bbh/phi1_5/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c287d4b-0319-4e4d-84ae-3eed0c443ec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/biz/Documents/research/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 927.09it/s]\n",
      "100%|████████████████████████████████████████| 187/187 [00:00<00:00, 927.77it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1006.90it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1039.76it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1038.82it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1026.31it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1042.09it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1037.69it/s]\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 678.36it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1030.80it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1039.65it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1043.75it/s]\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 997.81it/s]\n",
      "100%|███████████████████████████████████████| 146/146 [00:00<00:00, 1039.38it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1015.79it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1036.49it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1029.16it/s]\n",
      "100%|███████████████████████████████████████| 178/178 [00:00<00:00, 1028.21it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1040.42it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1031.77it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1031.59it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1025.40it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1032.17it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1039.59it/s]\n",
      "Checking cached requests: 100%|████████| 31710/31710 [00:01<00:00, 22840.39it/s]\n",
      "Running loglikelihood requests:   0%|                 | 0/31710 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 16\n",
      "Running loglikelihood requests: 100%|█████| 31710/31710 [07:20<00:00, 71.97it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=EleutherAI/pythia-1.4b), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (16)\n",
      "|                          Tasks                           |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|----------------------------------------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_bbh                                           |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_bbh_boolean_expressions                    |      1|none  |     3|acc_norm|↑  |0.5280|±  |0.0316|\n",
      "| - leaderboard_bbh_causal_judgement                       |      1|none  |     3|acc_norm|↑  |0.5134|±  |0.0366|\n",
      "| - leaderboard_bbh_date_understanding                     |      1|none  |     3|acc_norm|↑  |0.1920|±  |0.0250|\n",
      "| - leaderboard_bbh_disambiguation_qa                      |      1|none  |     3|acc_norm|↑  |0.3000|±  |0.0290|\n",
      "| - leaderboard_bbh_formal_fallacies                       |      1|none  |     3|acc_norm|↑  |0.4680|±  |0.0316|\n",
      "| - leaderboard_bbh_geometric_shapes                       |      1|none  |     3|acc_norm|↑  |0.0800|±  |0.0172|\n",
      "| - leaderboard_bbh_hyperbaton                             |      1|none  |     3|acc_norm|↑  |0.5120|±  |0.0317|\n",
      "| - leaderboard_bbh_logical_deduction_five_objects         |      1|none  |     3|acc_norm|↑  |0.2280|±  |0.0266|\n",
      "| - leaderboard_bbh_logical_deduction_seven_objects        |      1|none  |     3|acc_norm|↑  |0.1600|±  |0.0232|\n",
      "| - leaderboard_bbh_logical_deduction_three_objects        |      1|none  |     3|acc_norm|↑  |0.3400|±  |0.0300|\n",
      "| - leaderboard_bbh_movie_recommendation                   |      1|none  |     3|acc_norm|↑  |0.2640|±  |0.0279|\n",
      "| - leaderboard_bbh_navigate                               |      1|none  |     3|acc_norm|↑  |0.5800|±  |0.0313|\n",
      "| - leaderboard_bbh_object_counting                        |      1|none  |     3|acc_norm|↑  |0.1720|±  |0.0239|\n",
      "| - leaderboard_bbh_penguins_in_a_table                    |      1|none  |     3|acc_norm|↑  |0.2123|±  |0.0340|\n",
      "| - leaderboard_bbh_reasoning_about_colored_objects        |      1|none  |     3|acc_norm|↑  |0.1000|±  |0.0190|\n",
      "| - leaderboard_bbh_ruin_names                             |      1|none  |     3|acc_norm|↑  |0.2080|±  |0.0257|\n",
      "| - leaderboard_bbh_salient_translation_error_detection    |      1|none  |     3|acc_norm|↑  |0.2240|±  |0.0264|\n",
      "| - leaderboard_bbh_snarks                                 |      1|none  |     3|acc_norm|↑  |0.5393|±  |0.0375|\n",
      "| - leaderboard_bbh_sports_understanding                   |      1|none  |     3|acc_norm|↑  |0.5520|±  |0.0315|\n",
      "| - leaderboard_bbh_temporal_sequences                     |      1|none  |     3|acc_norm|↑  |0.2480|±  |0.0274|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_five_objects |      1|none  |     3|acc_norm|↑  |0.1960|±  |0.0252|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_seven_objects|      1|none  |     3|acc_norm|↑  |0.1560|±  |0.0230|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_three_objects|      1|none  |     3|acc_norm|↑  |0.3320|±  |0.0298|\n",
      "| - leaderboard_bbh_web_of_lies                            |      1|none  |     3|acc_norm|↑  |0.5120|±  |0.0317|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=EleutherAI/pythia-1.4b \\\n",
    "    --include_path ./yaml_configs/leaderboard_bbh.yaml \\\n",
    "    --tasks leaderboard_bbh \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_bbh/pythia-1_4/ \\\n",
    "    --output_path ./resulting_scores/learderboard_bbh/pythia-1_4/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe02de-b837-477f-abac-2caebf96efb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.02it/s]\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 931.18it/s]\n",
      "100%|████████████████████████████████████████| 187/187 [00:00<00:00, 939.13it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1000.07it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1035.59it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1041.38it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1028.87it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1046.23it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1049.58it/s]\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 671.48it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1032.53it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1038.82it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1045.00it/s]\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 999.16it/s]\n",
      "100%|███████████████████████████████████████| 146/146 [00:00<00:00, 1028.12it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1016.07it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1035.02it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1032.10it/s]\n",
      "100%|███████████████████████████████████████| 178/178 [00:00<00:00, 1026.17it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1040.02it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1040.05it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1034.07it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1029.05it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1036.78it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 1037.90it/s]\n",
      "Checking cached requests: 100%|████████| 31710/31710 [00:01<00:00, 22778.90it/s]\n",
      "Running loglikelihood requests:   0%|                 | 0/31710 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|█████| 31710/31710 [17:19<00:00, 30.49it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/mnt/spinning/beancounter_replication/continually_pretrained_models/pythia-1_4b-bc), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|                          Tasks                           |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|----------------------------------------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_bbh                                           |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_bbh_boolean_expressions                    |      1|none  |     3|acc_norm|↑  |0.5400|±  |0.0316|\n",
      "| - leaderboard_bbh_causal_judgement                       |      1|none  |     3|acc_norm|↑  |0.5134|±  |0.0366|\n",
      "| - leaderboard_bbh_date_understanding                     |      1|none  |     3|acc_norm|↑  |0.1680|±  |0.0237|\n",
      "| - leaderboard_bbh_disambiguation_qa                      |      1|none  |     3|acc_norm|↑  |0.3000|±  |0.0290|\n",
      "| - leaderboard_bbh_formal_fallacies                       |      1|none  |     3|acc_norm|↑  |0.4640|±  |0.0316|\n",
      "| - leaderboard_bbh_geometric_shapes                       |      1|none  |     3|acc_norm|↑  |0.1360|±  |0.0217|\n",
      "| - leaderboard_bbh_hyperbaton                             |      1|none  |     3|acc_norm|↑  |0.5160|±  |0.0317|\n",
      "| - leaderboard_bbh_logical_deduction_five_objects         |      1|none  |     3|acc_norm|↑  |0.2280|±  |0.0266|\n",
      "| - leaderboard_bbh_logical_deduction_seven_objects        |      1|none  |     3|acc_norm|↑  |0.1320|±  |0.0215|\n",
      "| - leaderboard_bbh_logical_deduction_three_objects        |      1|none  |     3|acc_norm|↑  |0.3440|±  |0.0301|\n",
      "| - leaderboard_bbh_movie_recommendation                   |      1|none  |     3|acc_norm|↑  |0.2680|±  |0.0281|\n",
      "| - leaderboard_bbh_navigate                               |      1|none  |     3|acc_norm|↑  |0.5800|±  |0.0313|\n",
      "| - leaderboard_bbh_object_counting                        |      1|none  |     3|acc_norm|↑  |0.0800|±  |0.0172|\n",
      "| - leaderboard_bbh_penguins_in_a_table                    |      1|none  |     3|acc_norm|↑  |0.2123|±  |0.0340|\n",
      "| - leaderboard_bbh_reasoning_about_colored_objects        |      1|none  |     3|acc_norm|↑  |0.1200|±  |0.0206|\n",
      "| - leaderboard_bbh_ruin_names                             |      1|none  |     3|acc_norm|↑  |0.2520|±  |0.0275|\n",
      "| - leaderboard_bbh_salient_translation_error_detection    |      1|none  |     3|acc_norm|↑  |0.2240|±  |0.0264|\n",
      "| - leaderboard_bbh_snarks                                 |      1|none  |     3|acc_norm|↑  |0.5393|±  |0.0375|\n",
      "| - leaderboard_bbh_sports_understanding                   |      1|none  |     3|acc_norm|↑  |0.4880|±  |0.0317|\n",
      "| - leaderboard_bbh_temporal_sequences                     |      1|none  |     3|acc_norm|↑  |0.2600|±  |0.0278|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_five_objects |      1|none  |     3|acc_norm|↑  |0.1720|±  |0.0239|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_seven_objects|      1|none  |     3|acc_norm|↑  |0.1560|±  |0.0230|\n",
      "| - leaderboard_bbh_tracking_shuffled_objects_three_objects|      1|none  |     3|acc_norm|↑  |0.3200|±  |0.0296|\n",
      "| - leaderboard_bbh_web_of_lies                            |      1|none  |     3|acc_norm|↑  |0.5120|±  |0.0317|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bradfordlevy/pythia-1.4b-bc-cp \\\n",
    "    --include_path yaml_configs/leaderboard_bbh.yaml \\\n",
    "    --tasks leaderboard_bbh \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_bbh/pythia-1_4b-bc/ \\\n",
    "    --output_path ./resulting_scores/learderboard_bbh/pythia-1_4b-bc/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7a614-3419-4b85-99b7-cc84494731e8",
   "metadata": {},
   "source": [
    "## compile bbh results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee8b0916-2da6-40e9-9625-ef81c50a7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi1_5_res = './resulting_scores/learderboard_bbh/phi1_5/microsoft__phi-1_5/results_2024-08-14T17-42-50.718050.json'\n",
    "phi1_5_cp_bc_res = './resulting_scores/learderboard_bbh/phi1_5_bc_cp/__mnt__spinning__beancounter_replication__continually_pretrained_models__phi-1_5-bc-cp-highlr-hf/results_2024-08-14T17-33-24.500762.json'\n",
    "pythia_res = './resulting_scores/learderboard_bbh/pythia-1_4/EleutherAI__pythia-1.4b/results_2024-08-15T00-07-39.589084.json'\n",
    "pythia_bc_res = './resulting_scores/learderboard_bbh/pythia-1_4b-bc/__mnt__spinning__beancounter_replication__continually_pretrained_models__pythia-1_4b-bc/results_2024-08-15T00-37-51.179928.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8960352-25dd-477c-86a9-a80643aa9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_accuracy(res_path):\n",
    "    with open(res_path) as outfile:\n",
    "        res = json.load(outfile)\n",
    "    acc_norms = []\n",
    "    for key, vals in res['results'].items():\n",
    "        if 'acc_norm,none' in vals:\n",
    "            acc_norms.append(vals['acc_norm,none'])\n",
    "    return sum(acc_norms)/len(acc_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ccec222-4690-456f-be8f-524ba485e0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33455561253143856"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(phi1_5_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7cc8ea51-4a1d-44a3-92e3-4cf01baf2349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3123777656052416"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(phi1_5_cp_bc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5b87a0e-e410-46ec-99fd-9fcfb7a57af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3173759830740473"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(pythia_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3b12ecd-60fb-40c4-a266-48a224e88ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31354264974071405"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(pythia_bc_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07bf60e-0e82-42e0-95dd-93e12eac179f",
   "metadata": {},
   "source": [
    "# MMLU-pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34dfce26-3265-459b-8c45-d294bbd6997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_leaderboard_mmlu_pro_string = '''\n",
    "dataset_path: TIGER-Lab/MMLU-Pro # a copy of `cais/leaderboard_mmlu` with no auxiliary_train split\n",
    "task: leaderboard_mmlu_pro\n",
    "test_split: test\n",
    "fewshot_split: validation\n",
    "fewshot_config:\n",
    "  sampler: first_n\n",
    "output_type: multiple_choice\n",
    "doc_to_text: !function utils.doc_to_text\n",
    "doc_to_choice: !function utils.doc_to_choice\n",
    "doc_to_target: answer\n",
    "metric_list:\n",
    "  - metric: acc\n",
    "    aggregation: mean\n",
    "    higher_is_better: true\n",
    "num_fewshot: 5\n",
    "metadata:\n",
    "  version: 0.1\n",
    "'''\n",
    "with open('./yaml_configs/leaderboard_mmlu_pro.yaml', 'w') as f:\n",
    "    f.write(YAML_leaderboard_mmlu_pro_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e29e98-d200-424c-8a5c-b439d0d22b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.08it/s]\n",
      "100%|██████████████████████████████████| 12032/12032 [00:00<00:00, 21990.42it/s]\n",
      "Checking cached requests: 100%|██████| 113995/113995 [00:05<00:00, 22128.61it/s]\n",
      "Running loglikelihood requests:   0%|                | 0/113995 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|██| 113995/113995 [15:23<00:00, 123.45it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/mnt/spinning/beancounter_replication/continually_pretrained_models/phi-1_5-bc-cp-highlr-hf), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|       Tasks        |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|--------------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|leaderboard_mmlu_pro|    0.1|none  |     5|acc   |↑  |0.1252|±  | 0.003|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bradfordlevy/phi-1_5-bc-cp \\\n",
    "    --include_path ./yaml_configs/leaderboard_mmlu_pro.yaml \\\n",
    "    --tasks leaderboard_mmlu_pro \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_mmlu_pro/phi1_5_bc_cp/ \\\n",
    "    --output_path ./resulting_scores/learderboard_mmlu_pro/phi1_5_bc_cp/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bce0e05d-5e9b-4522-8792-ebef926f5af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/biz/Documents/research/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████| 12032/12032 [00:00<00:00, 21668.29it/s]\n",
      "Checking cached requests: 100%|██████| 113995/113995 [00:08<00:00, 13302.86it/s]\n",
      "Running loglikelihood requests:   0%|                | 0/113995 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|██| 113995/113995 [05:35<00:00, 339.48it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=microsoft/phi-1_5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|       Tasks        |Version|Filter|n-shot|Metric|   |Value|   |Stderr|\n",
      "|--------------------|------:|------|-----:|------|---|----:|---|-----:|\n",
      "|leaderboard_mmlu_pro|    0.1|none  |     5|acc   |↑  | 0.17|±  |0.0034|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=microsoft/phi-1_5 \\\n",
    "    --include_path ./yaml_configs/leaderboard_mmlu_pro.yaml \\\n",
    "    --tasks leaderboard_mmlu_pro \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_mmlu_pro/phi1_5/ \\\n",
    "    --output_path ./resulting_scores/learderboard_mmlu_pro/phi1_5/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f7bf468-3046-4f92-8206-a812cc00ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/biz/Documents/research/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████| 12032/12032 [00:00<00:00, 21587.46it/s]\n",
      "Checking cached requests: 100%|██████| 113995/113995 [00:05<00:00, 19143.21it/s]\n",
      "Running loglikelihood requests:   0%|                | 0/113995 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|██| 113995/113995 [06:55<00:00, 274.32it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=EleutherAI/pythia-1.4b), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|       Tasks        |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|--------------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|leaderboard_mmlu_pro|    0.1|none  |     5|acc   |↑  |0.1125|±  |0.0029|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=EleutherAI/pythia-1.4b \\\n",
    "    --include_path ./yaml_configs/leaderboard_mmlu_pro.yaml \\\n",
    "    --tasks leaderboard_mmlu_pro \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_mmlu_pro/pythia-1_4/ \\\n",
    "    --output_path ./resulting_scores/learderboard_mmlu_pro/pythia-1_4/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495bd0f6-879e-466a-abeb-493667f8c170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|██████████████████████████████████| 12032/12032 [00:00<00:00, 22567.92it/s]\n",
      "Checking cached requests: 100%|██████| 113995/113995 [00:05<00:00, 22190.97it/s]\n",
      "Running loglikelihood requests:   0%|                | 0/113995 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 4\n",
      "Running loglikelihood requests: 100%|██| 113995/113995 [14:29<00:00, 131.05it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/mnt/spinning/beancounter_replication/continually_pretrained_models/pythia-1_4b-bc), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (4)\n",
      "|       Tasks        |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|--------------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|leaderboard_mmlu_pro|    0.1|none  |     5|acc   |↑  |0.1134|±  |0.0029|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bradfordlevy/pythia-1.4b-bc-cp \\\n",
    "    --include_path ./yaml_configs/leaderboard_mmlu_pro.yaml \\\n",
    "    --tasks leaderboard_mmlu_pro \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_mmlu_pro/pythia-1_4b-bc/ \\\n",
    "    --output_path ./resulting_scores/learderboard_mmlu_pro/pythia-1_4b-bc/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9d63b-0f86-4590-8cbc-4cc3b7335e87",
   "metadata": {},
   "source": [
    "# IFeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e55318f-8f75-43cf-99fc-e3b8d8465dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_leaderboard_ifeval_string = '''\n",
    "task: leaderboard_ifeval\n",
    "dataset_path: wis-k/instruction-following-eval\n",
    "dataset_name: null\n",
    "output_type: generate_until\n",
    "test_split: train\n",
    "num_fewshot: 0\n",
    "doc_to_text: prompt\n",
    "doc_to_target: 0\n",
    "generation_kwargs:\n",
    "  until: []\n",
    "  do_sample: false\n",
    "  temperature: 0.0\n",
    "  max_gen_toks: 1280\n",
    "process_results: !function utils.process_results\n",
    "metric_list:\n",
    "  - metric: prompt_level_strict_acc\n",
    "    aggregation: mean\n",
    "    higher_is_better: true\n",
    "  - metric: inst_level_strict_acc\n",
    "    aggregation: !function utils.agg_inst_level_acc\n",
    "    higher_is_better: true\n",
    "  - metric: prompt_level_loose_acc\n",
    "    aggregation: mean\n",
    "    higher_is_better: true\n",
    "  - metric: inst_level_loose_acc\n",
    "    aggregation: !function utils.agg_inst_level_acc\n",
    "    higher_is_better: true\n",
    "metadata:\n",
    "  version: 2.0\n",
    "fewshot_config:\n",
    "  sampler: first_n\n",
    "'''\n",
    "\n",
    "with open('leaderboard_ifeval.yaml', 'w') as f:\n",
    "    f.write(YAML_leaderboard_ifeval_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f58b00-4dae-43ae-9a4b-4dabef4eb09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.01it/s]\n",
      "100%|█████████████████████████████████████| 541/541 [00:00<00:00, 193991.49it/s]\n",
      "Checking cached requests: 100%|████████████| 541/541 [00:00<00:00, 21159.45it/s]\n",
      "Running generate_until requests:   0%|                  | 0/541 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\n",
      "Determined Largest batch size: 8\n",
      "Running generate_until requests: 100%|████████| 541/541 [24:32<00:00,  2.72s/it]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/mnt/spinning/beancounter_replication/continually_pretrained_models/phi-1_5-bc-cp-highlr-hf), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
      "|      Tasks       |Version|Filter|n-shot|        Metric         |   |Value |   |Stderr|\n",
      "|------------------|------:|------|-----:|-----------------------|---|-----:|---|------|\n",
      "|leaderboard_ifeval|      2|none  |     0|inst_level_loose_acc   |↑  |0.2554|±  |   N/A|\n",
      "|                  |       |none  |     0|inst_level_strict_acc  |↑  |0.2458|±  |   N/A|\n",
      "|                  |       |none  |     0|prompt_level_loose_acc |↑  |0.1386|±  |0.0149|\n",
      "|                  |       |none  |     0|prompt_level_strict_acc|↑  |0.1312|±  |0.0145|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bradfordlevy/phi-1_5-bc-cp \\\n",
    "    --include_path ./yaml_configs/leaderboard_ifeval.yaml \\\n",
    "    --tasks leaderboard_ifeval \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_ifeval/phi1_5_bc_cp/ \\\n",
    "    --output_path ./resulting_scores/learderboard_ifeval/phi1_5_bc_cp/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dd89a6c-3471-4078-9e08-d45e62a5e0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/biz/Documents/research/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████| 541/541 [00:00<00:00, 213303.11it/s]\n",
      "Checking cached requests: 100%|████████████| 541/541 [00:00<00:00, 13778.12it/s]\n",
      "Running generate_until requests:   0%|                  | 0/541 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\n",
      "Determined Largest batch size: 8\n",
      "Running generate_until requests: 100%|████████| 541/541 [12:47<00:00,  1.42s/it]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=microsoft/phi-1_5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
      "|      Tasks       |Version|Filter|n-shot|        Metric         |   |Value |   |Stderr|\n",
      "|------------------|------:|------|-----:|-----------------------|---|-----:|---|------|\n",
      "|leaderboard_ifeval|      2|none  |     0|inst_level_loose_acc   |↑  |0.2734|±  |   N/A|\n",
      "|                  |       |none  |     0|inst_level_strict_acc  |↑  |0.2698|±  |   N/A|\n",
      "|                  |       |none  |     0|prompt_level_loose_acc |↑  |0.1405|±  |0.0150|\n",
      "|                  |       |none  |     0|prompt_level_strict_acc|↑  |0.1368|±  |0.0148|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=microsoft/phi-1_5 \\\n",
    "    --include_path ./yaml_configs/leaderboard_ifeval.yaml \\\n",
    "    --tasks leaderboard_ifeval \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_ifeval/phi1_5/ \\\n",
    "    --output_path ./resulting_scores/learderboard_ifeval/phi1_5/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8bae855-5010-465d-a5db-2532bd02aab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/biz/Documents/research/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████| 541/541 [00:00<00:00, 121317.28it/s]\n",
      "Checking cached requests: 100%|████████████| 541/541 [00:00<00:00, 11394.30it/s]\n",
      "Running generate_until requests:   0%|                  | 0/541 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\n",
      "Determined Largest batch size: 8\n",
      "Running generate_until requests: 100%|████████| 541/541 [17:50<00:00,  1.98s/it]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=EleutherAI/pythia-1.4b), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
      "|      Tasks       |Version|Filter|n-shot|        Metric         |   |Value |   |Stderr|\n",
      "|------------------|------:|------|-----:|-----------------------|---|-----:|---|------|\n",
      "|leaderboard_ifeval|      2|none  |     0|inst_level_loose_acc   |↑  |0.3070|±  |   N/A|\n",
      "|                  |       |none  |     0|inst_level_strict_acc  |↑  |0.2986|±  |   N/A|\n",
      "|                  |       |none  |     0|prompt_level_loose_acc |↑  |0.1811|±  |0.0166|\n",
      "|                  |       |none  |     0|prompt_level_strict_acc|↑  |0.1719|±  |0.0162|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=EleutherAI/pythia-1.4b \\\n",
    "    --include_path ./yaml_configs/leaderboard_ifeval.yaml \\\n",
    "    --tasks leaderboard_ifeval \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_ifeval/pythia-1_4/ \\\n",
    "    --output_path ./resulting_scores/learderboard_ifeval/pythia-1_4/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62593664-d939-4a14-adc4-69ca4d78bc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.06it/s]\n",
      "100%|█████████████████████████████████████| 541/541 [00:00<00:00, 230180.41it/s]\n",
      "Checking cached requests: 100%|████████████| 541/541 [00:00<00:00, 23329.48it/s]\n",
      "Running generate_until requests:   0%|                  | 0/541 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\n",
      "Determined Largest batch size: 4\n",
      "Running generate_until requests: 100%|████████| 541/541 [36:51<00:00,  4.09s/it]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/mnt/spinning/beancounter_replication/continually_pretrained_models/pythia-1_4b-bc), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
      "|      Tasks       |Version|Filter|n-shot|        Metric         |   |Value |   |Stderr|\n",
      "|------------------|------:|------|-----:|-----------------------|---|-----:|---|------|\n",
      "|leaderboard_ifeval|      2|none  |     0|inst_level_loose_acc   |↑  |0.3022|±  |   N/A|\n",
      "|                  |       |none  |     0|inst_level_strict_acc  |↑  |0.2890|±  |   N/A|\n",
      "|                  |       |none  |     0|prompt_level_loose_acc |↑  |0.1830|±  |0.0166|\n",
      "|                  |       |none  |     0|prompt_level_strict_acc|↑  |0.1756|±  |0.0164|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bradfordlevy/pythia-1.4b-bc-cp \\\n",
    "    --include_path ./yaml_configs/leaderboard_ifeval.yaml \\\n",
    "    --tasks leaderboard_ifeval \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_ifeval/pythia-1_4b-bc/ \\\n",
    "    --output_path ./resulting_scores/learderboard_ifeval/pythia-1_4b-bc/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f026a-8270-48d4-acb7-ebc2175f19c8",
   "metadata": {},
   "source": [
    "# MUSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a012ac6-28fe-4b66-8d00-c7928d9647a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_leaderboard_musr_string = '''group: leaderboard_musr\n",
    "task:\n",
    "  - leaderboard_musr_murder_mysteries\n",
    "  - leaderboard_musr_object_placements\n",
    "  - leaderboard_musr_team_allocation\n",
    "'''\n",
    "\n",
    "with open('./yaml_configs/leaderboard_musr.yaml', 'w') as f:\n",
    "    f.write(YAML_leaderboard_musr_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba16ec-7422-4085-ac09-1f44e3d665c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.05it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 6674.07it/s]\n",
      "100%|███████████████████████████████████████| 256/256 [00:00<00:00, 6343.22it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 6477.93it/s]\n",
      "Checking cached requests: 100%|██████████| 2198/2198 [00:00<00:00, 11701.54it/s]\n",
      "Running loglikelihood requests:   0%|                  | 0/2198 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|███████| 2198/2198 [02:49<00:00, 12.94it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/mnt/spinning/beancounter_replication/continually_pretrained_models/phi-1_5-bc-cp-highlr-hf), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|                Tasks                |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|-------------------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_musr                     |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_musr_murder_mysteries |      1|none  |     0|acc_norm|↑  |0.5240|±  |0.0316|\n",
      "| - leaderboard_musr_object_placements|      1|none  |     0|acc_norm|↑  |0.2773|±  |0.0280|\n",
      "| - leaderboard_musr_team_allocation  |      1|none  |     0|acc_norm|↑  |0.2400|±  |0.0271|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bradfordlevy/phi-1_5-bc-cp \\\n",
    "    --include_path ./yaml_configs/leaderboard_musr.yaml \\\n",
    "    --tasks leaderboard_musr \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_musr/phi1_5_bc_cp/ \\\n",
    "    --output_path ./resulting_scores/learderboard_musr/phi1_5_bc_cp/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9846014e-b784-48b9-a0af-9403cc2d1025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/biz/Documents/research/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 6532.61it/s]\n",
      "100%|███████████████████████████████████████| 256/256 [00:00<00:00, 6422.94it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 6542.40it/s]\n",
      "Checking cached requests: 100%|██████████| 2198/2198 [00:00<00:00, 11562.40it/s]\n",
      "Running loglikelihood requests:   0%|                  | 0/2198 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|███████| 2198/2198 [01:00<00:00, 36.39it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=microsoft/phi-1_5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|                Tasks                |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|-------------------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_musr                     |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_musr_murder_mysteries |      1|none  |     0|acc_norm|↑  |0.4840|±  |0.0317|\n",
      "| - leaderboard_musr_object_placements|      1|none  |     0|acc_norm|↑  |0.2812|±  |0.0282|\n",
      "| - leaderboard_musr_team_allocation  |      1|none  |     0|acc_norm|↑  |0.2560|±  |0.0277|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=microsoft/phi-1_5 \\\n",
    "    --include_path ./yaml_configs/leaderboard_musr.yaml \\\n",
    "    --tasks leaderboard_musr \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_musr/phi1_5/ \\\n",
    "    --output_path ./resulting_scores/learderboard_musr/phi1_5/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e8bfa14-6a2c-417b-842e-8bc2fc9449f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/biz/Documents/research/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 6861.24it/s]\n",
      "100%|███████████████████████████████████████| 256/256 [00:00<00:00, 6654.90it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 6711.10it/s]\n",
      "Checking cached requests: 100%|██████████| 2198/2198 [00:00<00:00, 11549.11it/s]\n",
      "Running loglikelihood requests:   0%|                  | 0/2198 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|███████| 2198/2198 [01:24<00:00, 26.01it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=EleutherAI/pythia-1.4b), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|                Tasks                |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|-------------------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_musr                     |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_musr_murder_mysteries |      1|none  |     0|acc_norm|↑  |0.5000|±  |0.0317|\n",
      "| - leaderboard_musr_object_placements|      1|none  |     0|acc_norm|↑  |0.2773|±  |0.0280|\n",
      "| - leaderboard_musr_team_allocation  |      1|none  |     0|acc_norm|↑  |0.2720|±  |0.0282|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=EleutherAI/pythia-1.4b \\\n",
    "    --include_path ./yaml_configs/leaderboard_musr.yaml \\\n",
    "    --tasks leaderboard_musr \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_musr/pythia-1_4/ \\\n",
    "    --output_path ./resulting_scores/learderboard_musr/pythia-1_4/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e339a-4686-4713-91c4-b8f850b7211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.08it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 6924.90it/s]\n",
      "100%|███████████████████████████████████████| 256/256 [00:00<00:00, 6803.50it/s]\n",
      "100%|███████████████████████████████████████| 250/250 [00:00<00:00, 6871.54it/s]\n",
      "Checking cached requests: 100%|██████████| 2198/2198 [00:00<00:00, 18391.81it/s]\n",
      "Running loglikelihood requests:   0%|                  | 0/2198 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|███████| 2198/2198 [02:49<00:00, 12.95it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/mnt/spinning/beancounter_replication/continually_pretrained_models/pythia-1_4b-bc), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|                Tasks                |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|-------------------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_musr                     |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_musr_murder_mysteries |      1|none  |     0|acc_norm|↑  |0.5200|±  |0.0317|\n",
      "| - leaderboard_musr_object_placements|      1|none  |     0|acc_norm|↑  |0.2734|±  |0.0279|\n",
      "| - leaderboard_musr_team_allocation  |      1|none  |     0|acc_norm|↑  |0.3000|±  |0.0290|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bradfordlevy/pythia-1.4b-bc-cp \\\n",
    "    --include_path ./yaml_configs/leaderboard_musr.yaml \\\n",
    "    --tasks leaderboard_musr \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_musr/pythia-1_4b-bc/ \\\n",
    "    --output_path ./resulting_scores/learderboard_musr/pythia-1_4b-bc/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9e69668-3fdd-46ba-83d5-4832f852d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi1_5_res = './resulting_scores/learderboard_musr/phi1_5/microsoft__phi-1_5/results_2024-08-15T15-04-50.744667.json'\n",
    "phi1_5_cp_bc_res = './resulting_scores/learderboard_musr/phi1_5_bc_cp/__mnt__spinning__beancounter_replication__continually_pretrained_models__phi-1_5-bc-cp-highlr-hf/results_2024-08-15T15-01-16.452693.json'\n",
    "pythia_res = './resulting_scores/learderboard_musr/pythia-1_4/EleutherAI__pythia-1.4b/results_2024-08-15T15-11-25.678050.json'\n",
    "pythia_bc_res = './resulting_scores/learderboard_musr/pythia-1_4b-bc/__mnt__spinning__beancounter_replication__continually_pretrained_models__pythia-1_4b-bc/results_2024-08-15T15-30-02.650601.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b70743-a4ab-4918-b1d9-75e27f996f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34041666666666665"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(phi1_5_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c5f9c4-27aa-449b-8b19-6684924836e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34711458333333334"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(phi1_5_cp_bc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cb889ab-a410-4532-ae5e-3f3f3b589f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34978125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(pythia_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da4f21c-602c-4a3b-89e1-286594d42dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3644791666666667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(pythia_bc_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f31065-f592-4fb5-83c2-411f0282a99f",
   "metadata": {},
   "source": [
    "# GPQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "805ab494-1731-4ce4-98fd-731983261217",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_leaderboard_gpqa_string = '''\n",
    "group: leaderboard_gpqa\n",
    "task:\n",
    "  - leaderboard_gpqa_diamond\n",
    "  - leaderboard_gpqa_extended\n",
    "  - leaderboard_gpqa_main\n",
    "'''\n",
    "\n",
    "with open('leaderboard_gpqa.yaml', 'w') as f:\n",
    "    f.write(YAML_leaderboard_gpqa_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c1f6e-d049-4410-b766-551dd8122e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.07it/s]\n",
      "100%|███████████████████████████████████████| 198/198 [00:00<00:00, 3144.41it/s]\n",
      "100%|███████████████████████████████████████| 546/546 [00:00<00:00, 3376.76it/s]\n",
      "100%|███████████████████████████████████████| 448/448 [00:00<00:00, 3529.50it/s]\n",
      "Checking cached requests: 100%|██████████| 4768/4768 [00:00<00:00, 23614.40it/s]\n",
      "Running loglikelihood requests:   0%|                  | 0/4768 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|███████| 4768/4768 [01:38<00:00, 48.21it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/mnt/spinning/beancounter_replication/continually_pretrained_models/phi-1_5-bc-cp-highlr-hf), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|           Tasks            |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|----------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_gpqa            |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_gpqa_diamond |      1|none  |     0|acc_norm|↑  |0.2374|±  |0.0303|\n",
      "| - leaderboard_gpqa_extended|      1|none  |     0|acc_norm|↑  |0.2564|±  |0.0187|\n",
      "| - leaderboard_gpqa_main    |      1|none  |     0|acc_norm|↑  |0.2500|±  |0.0205|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bradfordlevy/phi-1_5-bc-cp \\\n",
    "    --include_path ./yaml_configs/leaderboard_gpqa.yaml \\\n",
    "    --tasks leaderboard_gpqa \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_gpqa/phi1_5_bc_cp/ \\\n",
    "    --output_path ./resulting_scores/learderboard_gpqa/phi1_5_bc_cp/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab2d7aba-c5c7-4f77-9139-3b333bb57175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/biz/Documents/research/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 198/198 [00:00<00:00, 3115.88it/s]\n",
      "100%|███████████████████████████████████████| 546/546 [00:00<00:00, 3278.15it/s]\n",
      "100%|███████████████████████████████████████| 448/448 [00:00<00:00, 3454.39it/s]\n",
      "Checking cached requests: 100%|██████████| 4768/4768 [00:00<00:00, 13723.19it/s]\n",
      "Running loglikelihood requests:   0%|                  | 0/4768 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|██████| 4768/4768 [00:37<00:00, 128.75it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=microsoft/phi-1_5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|           Tasks            |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|----------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_gpqa            |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_gpqa_diamond |      1|none  |     0|acc_norm|↑  |0.3030|±  |0.0327|\n",
      "| - leaderboard_gpqa_extended|      1|none  |     0|acc_norm|↑  |0.2399|±  |0.0183|\n",
      "| - leaderboard_gpqa_main    |      1|none  |     0|acc_norm|↑  |0.2768|±  |0.0212|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=microsoft/phi-1_5 \\\n",
    "    --include_path ./yaml_configs/leaderboard_gpqa.yaml \\\n",
    "    --tasks leaderboard_gpqa \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_gpqa/phi1_5/ \\\n",
    "    --output_path ./resulting_scores/learderboard_gpqa/phi1_5/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39cb10e1-e69a-4c6c-942c-2aa75200ea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/biz/Documents/research/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 198/198 [00:00<00:00, 3121.72it/s]\n",
      "100%|███████████████████████████████████████| 546/546 [00:00<00:00, 3371.51it/s]\n",
      "100%|███████████████████████████████████████| 448/448 [00:00<00:00, 3477.29it/s]\n",
      "Checking cached requests: 100%|██████████| 4768/4768 [00:00<00:00, 13532.79it/s]\n",
      "Running loglikelihood requests:   0%|                  | 0/4768 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running loglikelihood requests: 100%|██████| 4768/4768 [00:34<00:00, 138.97it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=EleutherAI/pythia-1.4b), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|           Tasks            |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|----------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_gpqa            |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_gpqa_diamond |      1|none  |     0|acc_norm|↑  |0.2475|±  |0.0307|\n",
      "| - leaderboard_gpqa_extended|      1|none  |     0|acc_norm|↑  |0.2399|±  |0.0183|\n",
      "| - leaderboard_gpqa_main    |      1|none  |     0|acc_norm|↑  |0.2612|±  |0.0208|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=EleutherAI/pythia-1.4b \\\n",
    "    --include_path ./yaml_configs/leaderboard_gpqa.yaml \\\n",
    "    --tasks leaderboard_gpqa \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_gpqa/pythia-1_4b/ \\\n",
    "    --output_path ./resulting_scores/learderboard_gpqa/pythia-1_4b/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981fa43-276a-41bc-a74a-c8fb524482dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.05it/s]\n",
      "100%|███████████████████████████████████████| 198/198 [00:00<00:00, 3132.97it/s]\n",
      "100%|███████████████████████████████████████| 546/546 [00:00<00:00, 3418.92it/s]\n",
      "100%|███████████████████████████████████████| 448/448 [00:00<00:00, 3511.22it/s]\n",
      "Checking cached requests: 100%|██████████| 4768/4768 [00:00<00:00, 23441.37it/s]\n",
      "Running loglikelihood requests:   0%|                  | 0/4768 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 4\n",
      "Running loglikelihood requests: 100%|███████| 4768/4768 [01:26<00:00, 55.16it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/mnt/spinning/beancounter_replication/continually_pretrained_models/pythia-1_4b-bc), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (4)\n",
      "|           Tasks            |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|----------------------------|-------|------|-----:|--------|---|-----:|---|-----:|\n",
      "|leaderboard_gpqa            |    N/A|      |      |        |   |      |   |      |\n",
      "| - leaderboard_gpqa_diamond |      1|none  |     0|acc_norm|↑  |0.2727|±  |0.0317|\n",
      "| - leaderboard_gpqa_extended|      1|none  |     0|acc_norm|↑  |0.2729|±  |0.0191|\n",
      "| - leaderboard_gpqa_main    |      1|none  |     0|acc_norm|↑  |0.2567|±  |0.0207|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=bradfordlevy/pythia-1.4b-bc-cp \\\n",
    "    --include_path ./yaml_configs/leaderboard_gpqa.yaml \\\n",
    "    --tasks leaderboard_gpqa \\\n",
    "    --batch_size auto \\\n",
    "    --use_cache ./resulting_scores/learderboard_gpqa/pythia-1_4b_bc/ \\\n",
    "    --output_path ./resulting_scores/learderboard_gpqa/pythia-1_4b_bc/ \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e35f013d-a4ed-409e-bade-4e6b8f7498cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi1_5_res = './resulting_scores/learderboard_gpqa/phi1_5/microsoft__phi-1_5/results_2024-08-15T16-04-07.002330.json'\n",
    "phi1_5_cp_bc_res = './resulting_scores/learderboard_gpqa/phi1_5_bc_cp/__mnt__spinning__beancounter_replication__continually_pretrained_models__phi-1_5-bc-cp-highlr-hf/results_2024-08-15T16-02-56.861924.json' \n",
    "pythia_res = './resulting_scores/learderboard_gpqa/pythia-1_4b/EleutherAI__pythia-1.4b/results_2024-08-15T16-05-24.665140.json'\n",
    "pythia_bc_res = './resulting_scores/learderboard_gpqa/pythia-1_4b_bc/__mnt__spinning__beancounter_replication__continually_pretrained_models__pythia-1_4b-bc/results_2024-08-15T17-15-19.349928.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02b34852-e405-496e-a454-f2e595a24f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27324758574758573"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(phi1_5_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3758582-4a74-48f8-9d28-792d90ef7d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2479279979279979"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(phi1_5_cp_bc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb08b86-3f71-47d4-a5f0-1424a8e38466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24952073389573393"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(pythia_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b47c92f3-8102-4a67-85de-e6c1b2dcfe70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.267439158064158"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(pythia_bc_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
