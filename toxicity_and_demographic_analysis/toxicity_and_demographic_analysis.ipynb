{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46bbcd02-d82c-4822-b55d-4abd09f260dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from googleapiclient import discovery\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import gzip\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f94b3-d877-48cf-a2d3-187ecfb24bb8",
   "metadata": {},
   "source": [
    "# Download C4-en dataset (305GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d9789-b442-4799-bd57-d0d40af2705f",
   "metadata": {},
   "source": [
    "Only run the following command to download if you have sufficient storage for the C4-en dataset, which is 305GB.\\\n",
    "If not, we provide a dataset of the extracted descriptor sentences in C4-en which is 11GB in '/supporting_datasets/c4_sentences_w_descriptors'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9675b34-c5fd-4049-8817-bb904a97a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/datasets/allenai/c4\n",
    "!cd c4\n",
    "!git lfs pull --include \"en/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d387c2f4-c518-4b6f-8c8e-e30943397fce",
   "metadata": {},
   "source": [
    "# Download BeanCounter dataset (train + val = 170GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7320d40-4408-4712-8a9f-9a171135611a",
   "metadata": {},
   "source": [
    "Only run the following command to download if you have sufficient storage\\\n",
    "-- train split: 142 GB \\\n",
    "-- val split: 25MB \\\n",
    "-- sample split: 1.5GB \\\n",
    "-- deduped split: 60GB \\\n",
    "-- fraud split: 411MB \\\n",
    "You can choose to download a specific split by using --include \"split_name/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e0060-772b-4126-81bc-020cf964598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/datasets/bradfordlevy/BeanCounter\n",
    "!cd BeanCounter\n",
    "!git lfs pull --include \"sample/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a041a",
   "metadata": {},
   "source": [
    "# Download extracted descriptor sentences dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec6ccd",
   "metadata": {},
   "source": [
    "If you want to explore the extracted sentences from BeanCounter and C4 with demographic descriptors, you can download the dataset with the commands below \\\n",
    "-- default split \\\n",
    "-- bc-clean split \\\n",
    "-- c4-en split \\\n",
    "-- sample split \\\n",
    "You can choose to download a specific split by using --include \"split_name/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01889172",
   "metadata": {},
   "outputs": [],
   "source": [
    "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/datasets/bradfordlevy/BeanCounter-Descriptor-Sents\n",
    "!cd BeanCounter-Descriptor_sents\n",
    "!git lfs pull --include \"sample/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23baef-e6e5-4cea-ad65-79c0a8606e59",
   "metadata": {},
   "source": [
    "# Set up necessary directories & Perspective API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc6287-1f71-4f58-b103-d56155f26bd1",
   "metadata": {},
   "source": [
    "Perspective API Key is necessary if want to process toxicity of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7520c47-7652-4791-9fe8-5179754a469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTING_DATA_DIR = '/supporting_datasets'\n",
    "BC_DATASET_DIR = 'beancounter'\n",
    "TRAIN_SPLIT = 'train'\n",
    "VAL_SPLIT = 'validation'\n",
    "C4_DIR = 'c4/en'\n",
    "BC_EXTRACTED_SENTENCES_DIR = 'beancounter_extracted_descriptor_sentences' # change directory name to where sentences are stored\n",
    "C4_EXTRACTED_SENTENCES_DIR = 'c4_sentences_w_descriptors' # change directory name to where sentences are stored\n",
    "BC_SENTENCE_BATCHES_DIR = 'beancounter_sentences_batches'\n",
    "C4_SENTENCE_BATCHES_DIR = 'c4_sentence_batches'\n",
    "C4_PERSPECTIVE_SENTENCE_BATCHES_DIR = 'c4_perspective_sentence_batches'\n",
    "DESCRIPTOR_TO_COUNT_CSV = 'beancounter_descriptor_count.csv'\n",
    "BC_PERSPECTIVE_SCORES = 'beancounter_perspective_scores'\n",
    "C4_PERSPECTIVE_SCORES = 'c4_perspective_scores'\n",
    "current_date = str(date.today()) # for naming output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285ae663-4af6-48e7-b990-d6e76cdb49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSPECTIVE_API_KEY = '' # Perspective API key; set up Perspective API here: https://developers.perspectiveapi.com/s/docs-get-started?language=en_US\n",
    "PERSPECTIVE_API_QPS = 20 # default is 1 query per second (QPS), you can increase the QPS by submitting a request via: https://developers.perspectiveapi.com/s/request-quota-increase?language=en_US"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943070c0-48a7-41b1-a16e-2792e4732042",
   "metadata": {},
   "source": [
    "# Load all demographic descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "192c32de-3665-458a-80af-f472918f427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('demographic_descriptors.json', 'r') as f:\n",
    "    tox_analysis_descriptors = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de3951-2add-4092-8cbb-708e589495c1",
   "metadata": {},
   "source": [
    "# Load broad categories associated with each descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f7fb615-26ab-467f-9074-8ae3148fb591",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('descriptors_to_categories.json', 'r') as f:\n",
    "    desc2category = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b2650-9941-4474-b8bf-9db058a25363",
   "metadata": {},
   "source": [
    "# Create dictionary mapping a variant descriptor to the descriptor's original form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a25a624-731b-4d9f-a08b-c02b5fd3ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "variation2original = {}\n",
    "for desc in tox_analysis_descriptors:\n",
    "    if '-' in desc:\n",
    "        variation2original[desc.replace('-', ' ')] = desc\n",
    "    elif ' ' in desc:\n",
    "        variation2original[desc.replace(' ', '-')] = desc\n",
    "    variation2original[desc] = desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2b936-32cd-44dc-b1f2-c9627333bfe8",
   "metadata": {},
   "source": [
    "# Utils for extracting sentences with descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "293504f5-3d6b-4c4d-abae-87fa667d9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.find(\"tokenizers/punkt\")\n",
    "sentence_tokenizer = nltk.data.load(\"nltk:tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "734e59dc-c420-4aa3-bfd4-b457de8cb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_w_descriptors(cleaned_text, sentence_tokenizer=sentence_tokenizer, tox_analysis_descriptors = tox_analysis_descriptors):\n",
    "    tox_desc = '|'.join(tox_analysis_descriptors)\n",
    "    re_desc = re.compile(\"\"\"(?=(?:([^\\w\\s]|\\s)(\"\"\" + tox_desc +\"\"\")([^\\w\\s]|\\s)))\"\"\")\n",
    "    \n",
    "    lines = cleaned_text.splitlines()\n",
    "    sentences = [sentence_tokenizer.tokenize(line) for line in lines]\n",
    "    sentences = [c for b in sentences for c in b]\n",
    "    sentences_tbp = defaultdict(set)\n",
    "    for sent in sentences:\n",
    "        matches = re.findall(re_desc, ' ' + sent)\n",
    "        for m in matches:\n",
    "            sentences_tbp[m[1]].add(sent)\n",
    "\n",
    "    return sentences_tbp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217127c6-4902-4de3-8bd8-cb79f3d7eff5",
   "metadata": {},
   "source": [
    "# Extracting sentences containing descriptors from BeanCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7714820f-091f-478d-b7bf-108681e5a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files containing the BeanCounter dataset\n",
    "files_to_process = []\n",
    "for path, dirs, fns in os.walk(os.path.join(SUPPORTING_DATA_DIR, BC_DATASET_DIR, TRAIN_SPLIT)):\n",
    "    for fn in fns:\n",
    "        if os.path.splitext(fn)[-1] == '.gz':\n",
    "            files_to_process.append(os.path.join(path, fn))\n",
    "\n",
    "for path, dirs, fns in os.walk(os.path.join(SUPPORTING_DATA_DIR, BC_DATASET_DIR, VAL_SPLIT)):\n",
    "    for fn in fns:\n",
    "        if os.path.splitext(fn)[-1] == '.gz':\n",
    "            files_to_process.append(os.path.join(path, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3958f098-0a4a-4ceb-bfff-e96a883e10c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define output directory for extracted sentences\n",
    "out_dir = os.path.join(SUPPORTING_DATA_DIR, f\"{BC_EXTRACTED_SENTENCES_DIR}_{current_date}\")\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f4250d3b-2274-419f-aec9-66f9e3f10de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def work(path, out_dir = out_dir):\n",
    "    force = False\n",
    "    path_out = os.path.join(out_dir, path.split('/')[-2])\n",
    "    fn = path.rsplit('/', 1)[-1]\n",
    "    \n",
    "    if os.path.exists(os.path.join(path_out, fn)) and not force:\n",
    "        return False\n",
    "\n",
    "    os.makedirs(path_out, exist_ok = True)\n",
    "    \n",
    "    # write .lock file; files that did not finish processing will have .lock extension.\n",
    "    lock_file = os.path.join(path_out, fn.rsplit('.', 1)[0])+'.lock'\n",
    "    open(lock_file, 'w').close()\n",
    "    \n",
    "    with gzip.open(path, 'r') as filings:\n",
    "        all_filings = []\n",
    "        for line in filings.readlines():\n",
    "            info = json.loads(line)\n",
    "            desc2sentences = get_sentences_w_descriptors(info['text'])\n",
    "\n",
    "            for descriptor, sentences in desc2sentences.items():\n",
    "                for sent in sentences:\n",
    "                    desc_dict = {}\n",
    "                    desc_dict['accession'] = info['accession']\n",
    "                    desc_dict['filename'] = info['filename']\n",
    "                    desc_dict['type_filing'] = info['type_filing']\n",
    "                    desc_dict['type_attachment'] = info['type_attachment']\n",
    "                    desc_dict['form_type'] = info['form_type']\n",
    "                    desc_dict['date'] = info['ts_accept'].split('T')[0]\n",
    "                    desc_dict['descriptor'] = descriptor\n",
    "                    desc_dict['sentence'] = sent\n",
    "                    all_filings.append(desc_dict)\n",
    "        \n",
    "        with gzip.open(os.path.join(path_out, fn), 'w') as out_file:\n",
    "            for filing in all_filings:\n",
    "                out_file.write((json.dumps(filing) + '\\n').encode())\n",
    "\n",
    "    # remove .lock file\n",
    "    os.remove(lock_file)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3468667c-baae-49b4-a14c-359e0418b9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1acfe61e56145b8a63fa9121dfc32f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with mp.Pool(16) as p: # set workers as 8 or 16\n",
    "    results = [r for r in tqdm(p.imap_unordered(work, files_to_process), total=len(files_to_process), miniters=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f55339-ccd9-48b7-a7e4-511f5c16eefb",
   "metadata": {},
   "source": [
    "# Extracting sentences containing descriptors from C4-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "319ed0f0-217e-49bf-83c3-5790b7670cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_process = []\n",
    "for path, dirs, fns in os.walk(os.path.join(C4_DIR)):\n",
    "    for fn in fns:\n",
    "        if os.path.splitext(fn)[-1] == '.gz':\n",
    "            files_to_process.append(os.path.join(path, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c994def9-e665-48d5-80d1-db34db08f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(SUPPORTING_DATA_DIR, f\"{C4_EXTRACTED_SENTENCES_DIR}_{current_date}\")\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ee5f950b-2245-40d0-b8f0-e97371049eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def work(path, out_dir = out_dir):\n",
    "    force = False\n",
    "    fn = path.rsplit('/', 1)[1]\n",
    "    \n",
    "    if os.path.exists(os.path.join(out_dir, fn)) and not force:\n",
    "        return False\n",
    "    \n",
    "    # write .lock file; files that did not finish processing will have .lock extension.\n",
    "    lock_file = os.path.join(out_dir, fn.rsplit('.', 1)[0])+'.lock'\n",
    "    open(lock_file, 'w').close()\n",
    "    \n",
    "    with gzip.open(path, 'r') as entries:\n",
    "        all_entries = []\n",
    "        for entry in entries:\n",
    "            info = json.loads(entry)\n",
    "            \n",
    "            desc2sentences = get_sentences_w_descriptors(info['text'])\n",
    "\n",
    "            for descriptor, sentences in desc2sentences.items():\n",
    "                for sent in sentences:\n",
    "                    desc_dict = {}\n",
    "                    desc_dict['url'] = info['url']\n",
    "                    desc_dict['timestamp'] = info['timestamp']\n",
    "                    desc_dict['descriptor'] = descriptor\n",
    "                    desc_dict['sentence'] = sent\n",
    "                    all_entries.append(desc_dict)\n",
    "        \n",
    "        with gzip.open(os.path.join(out_dir, fn), 'w') as out_file:\n",
    "            for entry in all_entries:\n",
    "                out_file.write((json.dumps(entry) + '\\n').encode())\n",
    "\n",
    "    # remove .lock file.\n",
    "    os.remove(lock_file)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de499b-8cfe-4b07-a325-6f212e3d489b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c32d2e0660485ca5c13ffd55523b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1032 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with mp.Pool(16) as p: \n",
    "    results = [r for r in tqdm(p.imap_unordered(work, files_to_process), total=len(files_to_process), miniters=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6bf03-771a-4988-a816-01be3f3d881c",
   "metadata": {},
   "source": [
    "# Separate BeanCounter sentences into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "105009bc-ac17-46ba-95cc-e5019ecb8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_path = os.path.join(SUPPORTING_DATA_DIR, BC_EXTRACTED_SENTENCES_DIR)\n",
    "def get_entries_generator(root_directory):\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            if os.path.splitext(file_path)[-1] == '.gz':\n",
    "                with gzip.open(file_path, 'r') as filings:\n",
    "                    for filing in filings.readlines():\n",
    "                        yield json.loads(filing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "162f78d4-9ae5-436f-bfb0-661554dcdb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filings_generator = get_entries_generator(observations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cdf2510d-50e7-4fc8-a604-e308ecd866d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory for batched sentences\n",
    "out_dir = os.path.join(SUPPORTING_DATA_DIR, f\"{BC_SENTENCE_BATCHES_DIR}_{current_date}\")\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a3f70772-835f-4810-b993-07d8a252918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batches of 10_000 sentences. Remove duplicate sentences.\n",
    "seen = set()\n",
    "batch_size = 10_000\n",
    "batch = []\n",
    "batch_num = 0\n",
    "\n",
    "for filing in tqdm(all_filings_generator):\n",
    "    accession = filing['accession']\n",
    "    filename = filing['filename']\n",
    "    form_type = filing['form_type']\n",
    "    attachment_type = filing['type_attachment']\n",
    "    type_filing = filing['type_filing']\n",
    "    sent = filing['sentence']\n",
    "    desc = filing['descriptor']\n",
    "    accepted_date = filing['date']\n",
    "    text_hash = hashlib.md5(sent.encode()).hexdigest()\n",
    "    if text_hash in seen:\n",
    "        continue\n",
    "    seen.add(text_hash)\n",
    "    if len(batch) < batch_size:\n",
    "        batch.append({'accession': accession, 'filename': filename, 'descriptor': desc, 'sentence': sent, \\\n",
    "                      'text_hash': text_hash, 'date': accepted_date, 'form_type': form_type, 'type_attachment': attachment_type, 'type_filing': type_filing})\n",
    "    else:\n",
    "        with gzip.open(os.path.join(out_dir, 'batch_%s.gz' % batch_num), 'w') as out_file:\n",
    "            for filing in batch:\n",
    "                out_file.write((json.dumps(filing) + '\\n').encode())\n",
    "        batch_num += 1\n",
    "        batch = [{'accession': accession, 'filename': filename, 'descriptor': desc, 'sentence': sent, \\\n",
    "                      'text_hash': text_hash, 'date': accepted_date, 'form_type': form_type, 'type_attachment': attachment_type, 'type_filing': type_filing}]\n",
    "\n",
    "with gzip.open(os.path.join(out_dir, 'batch_%s.gz' % batch_num), 'w') as out_file:\n",
    "    for filing in batch:\n",
    "        out_file.write((json.dumps(filing) + '\\n').encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaed0fb-18ec-463c-97fa-c9f99e094480",
   "metadata": {},
   "source": [
    "# Perspective analysis of BeanCounter sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4134e-e56d-4071-ad5e-952c57b08954",
   "metadata": {},
   "source": [
    "Running this analysis on a QPS of 20 with 19 workers in multiprocessing took ~5 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1078ebd3-10ad-4196-88e4-a47869ce21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(root_directory):\n",
    "    all_batch_paths = []\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            if os.path.splitext(file_path)[-1] == '.gz':\n",
    "                all_batch_paths.append(file_path)\n",
    "    return all_batch_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e49185bf-312b-48ee-b830-7c6f8d8ddacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_paths = get_batches(os.path.join(SUPPORTING_DATA_DIR, f'{BC_SENTENCE_BATCHES_DIR}_{current_date}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e904c50-f8b9-4d04-98e9-98bd887c7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(cleaned_text, max_char_length):\n",
    "    if len(cleaned_text) <= max_char_length:\n",
    "        return [cleaned_text]\n",
    "    lines = iter(cleaned_text.splitlines())\n",
    "    chunks, current = [], next(lines)\n",
    "    for l in lines:\n",
    "        if len(current) + len(l) > max_char_length:\n",
    "            if len(current) > max_char_length:\n",
    "                current_chunks = [current[i:i + max_char_length] for i in range(0, len(current), max_char_length)]\n",
    "                chunks+= current_chunks\n",
    "                current = l\n",
    "                continue\n",
    "            chunks.append(current)\n",
    "            current = l\n",
    "        else:\n",
    "            current += '\\n' + l\n",
    "\n",
    "    # chunk the last line\n",
    "    if len(current) < max_char_length:\n",
    "        chunks.append(current)\n",
    "    else:\n",
    "        chunks += [current[i:i + max_char_length] for i in range(0, len(current), max_char_length)]\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c648f09c-d6f2-4e74-a681-2160c02df2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char_length = 18_000 # perspective api takes at most 20KB\n",
    "def get_perspective_toxicity(text, client, max_char_length = max_char_length):\n",
    "    \"\"\"\n",
    "    parameters: text (str), client (perspective api client)\n",
    "    returns: dictionary mapping the chunk to associated toxicity score\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = chunking(text, max_char_length)\n",
    "    chunks2score = {}\n",
    "    \n",
    "    for c in chunks:\n",
    "        if c.strip():\n",
    "            start = time.time()\n",
    "            analyze_request = {\n",
    "                'comment': {'text': c.strip()},\n",
    "                'languages': ['en'],\n",
    "                'requestedAttributes': {'TOXICITY': {}}\n",
    "            }\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "            end = time.time()\n",
    "            sleep_time = 1-(end-start)\n",
    "            time.sleep(max(sleep_time, 0))\n",
    "            chunks2score[c.strip()] = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "    return chunks2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39aaccc6-0e59-43fe-b217-e1e354e0b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def work(sentence):\n",
    "    # build client here\n",
    "    client = discovery.build(\n",
    "        \"commentanalyzer\",\n",
    "        \"v1alpha1\",\n",
    "        developerKey=PERSPECTIVE_API_KEY,\n",
    "        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "        static_discovery=False,\n",
    "        )\n",
    "\n",
    "    sentence_hash = hashlib.md5(sentence.encode()).hexdigest()\n",
    "    tox_score = list(get_perspective_toxicity(sentence, client).values())[0]\n",
    "    return {sentence_hash:tox_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e241dd-8b3a-4090-b565-a593f7913086",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(SUPPORTING_DATA_DIR, f'{BC_PERSPECTIVE_SCORES}_{current_date}')\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65b1698c-fc72-4405-a969-60be3eefb1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_toxicity_scores(batch_paths, out_dir = out_dir):\n",
    "    for batch in tqdm(batch_paths):\n",
    "        res_path = os.path.join(out_dir, batch.rsplit('/', 1)[1].split('.')[0] + '.pkl')\n",
    "\n",
    "        if os.path.exists(res_path):\n",
    "            with open(res_path, 'rb') as res_file:\n",
    "                res_dict = pickle.load(res_file)\n",
    "        else:\n",
    "            res_dict = dict()\n",
    "    \n",
    "        all_sent2hash= dict()\n",
    "\n",
    "        with gzip.open(batch, 'r') as batch_n:\n",
    "            for sent_info in batch_n.readlines():\n",
    "                info = json.loads(sent_info)\n",
    "                sent = info['sentence']\n",
    "                if len(sent.encode(\"utf8\")) > 20_000:\n",
    "                    continue\n",
    "                sent_hash = info['text_hash']\n",
    "                all_sent2hash[sent_hash] = sent\n",
    "    \n",
    "        to_process = [sent for hash, sent in all_sent2hash.items() if hash not in res_dict]\n",
    "        if len(to_process) == 0:\n",
    "            continue\n",
    "\n",
    "        n_workers = PERSPECTIVE_API_QPS-1\n",
    "        try:\n",
    "            with mp.Pool(n_workers) as p:\n",
    "                results = [r for r in tqdm(p.imap_unordered(work, to_process), total=len(to_process), miniters=1)]\n",
    "        except:\n",
    "            n_workers = n_workers//2\n",
    "            with mp.Pool(n_workers) as p:\n",
    "                results = [r for r in tqdm(p.imap_unordered(work, to_process), total=len(to_process), miniters=1)]\n",
    "       \n",
    "        for r in results:\n",
    "            if isinstance(r, dict):\n",
    "                res_dict.update(r)\n",
    "    \n",
    "        with open(res_path, 'wb') as out_file:\n",
    "            pickle.dump(res_dict, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8bc05b8-f04f-4f02-924c-60d32a14759b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dcad3c8b514e46ab0c0cefbfff1d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_toxicity_scores(batch_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39e9f5-0433-4b68-a037-3d4b2eeddaac",
   "metadata": {},
   "source": [
    "# Get descriptors and its count from BeanCounter in order to sample c4-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b1b8295-25d7-4b1c-b303-77a5316532d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e29a08f3494cb2bd751976b21926e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get scores from BeanCounter's perspective analysis\n",
    "scores_tbp = []\n",
    "for path, dirs, fns in os.walk(os.path.join(SUPPORTING_DATA_DIR, f'{BC_PERSPECTIVE_SCORES}_{current_date}')): # change to correct path if necessary\n",
    "    for fn in fns:\n",
    "        if os.path.splitext(fn)[-1] == '.pkl':\n",
    "            scores_tbp.append(os.path.join(path, fn))\n",
    "\n",
    "list_score_dfs = []\n",
    "for path in tqdm(scores_tbp):\n",
    "    with open(path, 'rb') as out_file:\n",
    "        score_df = pd.DataFrame(pickle.load(out_file).items()).rename(columns={0: 'text_hash', 1: 'toxicity_score'})\n",
    "        list_score_dfs.append(score_df)\n",
    "\n",
    "all_scores_updated = pd.concat(list_score_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1e41fb6-48ca-462d-af21-6ed302569468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db5532ed7c9467e9077708fb608ae67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all extracted sentences from BeanCounter\n",
    "files_to_process = []\n",
    "for path, dirs, fns in os.walk(os.path.join(SUPPORTING_DATA_DIR, f'{BC_SENTENCE_BATCHES_DIR}_{current_date}')): # change to correct path if necessary\n",
    "    for fn in fns:\n",
    "        if os.path.splitext(fn)[-1] == '.gz':\n",
    "            files_to_process.append(os.path.join(path, fn))\n",
    "\n",
    "list_dfs = []\n",
    "for path in tqdm(files_to_process):\n",
    "    with gzip.open(path, 'r') as out_file:\n",
    "        sents_df = pd.read_json(out_file, lines=True, orient='records')\n",
    "        list_dfs.append(sents_df)\n",
    "\n",
    "all_sents_df = pd.concat(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14524441-840f-4322-8829-e0a1c89c3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents_df['desc_len'] = all_sents_df['descriptor'].str.len()\n",
    "all_sents_df = all_sents_df.sort_values(by='desc_len', ascending = False).groupby('text_hash').head(1) # if a sentence contains multiple descriptors, only keep the row with the longest descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04fcdf79-84d2-41da-a6b6-1a3db70cc65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2score = all_scores_updated.merge(\n",
    "    all_sents_df,\n",
    "    on = 'text_hash',\n",
    "    how = 'left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d621e2-6510-47d4-ae17-475a53d629f4",
   "metadata": {},
   "source": [
    "## Descriptor mapped to number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dc56075-2d42-40a9-b181-eee934e08f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit analysis to years 1996-2023 \n",
    "sent2score['year'] = sent2score['date'].apply(lambda x:int(str(x).split('-')[0]))\n",
    "sent2score = sent2score.loc[(sent2score['year'] >= 1996) & (sent2score['year'] <= 2023)]\n",
    "sent2score = sent2score.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b587ee6-f519-4932-83d1-87f4c312c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc2count_sample = pd.DataFrame(sent2score.groupby('descriptor')['text_hash'].count())\n",
    "desc2count_sample = desc2count_sample.reset_index().rename(columns = {'text_hash':'num_unique_sentences'})\n",
    "desc2count_sample = desc2count_sample.sort_values(by='num_unique_sentences', ascending = False)\n",
    "desc2count_sample['original_descriptor'] = desc2count_sample['descriptor'].apply(lambda x : variation2original[x] if x in variation2original.keys() else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b95770a2-e108-4e95-bd53-2787df5af447",
   "metadata": {},
   "outputs": [],
   "source": [
    "BC_DESCRIPTOR_COUNT_CSV = f'beancounter_descriptor_count_{current_date}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b205a53-cfbd-44bf-a416-aab486fbaf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc2count_sample.to_csv(os.path.join(SUPPORTING_DATA_DIR, BC_DESCRIPTOR_COUNT_CSV))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3f68a-1fab-4657-a8cb-320e146a6256",
   "metadata": {},
   "source": [
    "# Batch C4 sentences (in preparation for sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa1b595-987c-424a-b8ca-0f9e03a63eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_path = os.path.join(SUPPORTING_DATA_DIR, f'{C4_EXTRACTED_SENTENCES_DIR}_{current_date}')\n",
    "def get_entries_generator(root_directory):\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            if os.path.splitext(file_path)[-1] == '.gz':\n",
    "                with gzip.open(file_path, 'r') as filings:\n",
    "                    for filing in filings.readlines():\n",
    "                        yield json.loads(filing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74948ac1-4d81-4219-93d8-a61d93e31d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entries = get_entries_generator(obs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7547dd93-f2aa-474f-80f1-68730feb1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(SUPPORTING_DATA_DIR, f'{C4_SENTENCE_BATCHES_DIR}_{current_date}') \n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ddd4153-0d35-49d8-8f3f-22bd789c8a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049c80460b8d44b79ab3e6212a81178b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seen = set()\n",
    "batch_size = 10_000\n",
    "batch = []\n",
    "batch_num = 0\n",
    "count = 0\n",
    "\n",
    "for filing in tqdm(all_entries):\n",
    "    entry_url = filing['url']\n",
    "    entry_time = filing['timestamp']\n",
    "    if 'sentences_w_descriptors' in filing:\n",
    "        for desc, sent_list in filing['sentences_w_descriptors'].items():\n",
    "            for sent in sent_list:\n",
    "                text_hash = hashlib.md5(sent.encode()).hexdigest()\n",
    "                if text_hash in seen:\n",
    "                    continue\n",
    "                seen.add(text_hash)\n",
    "                if len(batch) < batch_size:\n",
    "                    batch.append({'url': entry_url, 'timestamp': entry_time, 'descriptor': desc, 'sentence': sent, 'text_hash': text_hash})\n",
    "                else:\n",
    "                    with gzip.open(os.path.join(out_dir, 'batch_%s.gz' % batch_num), 'w') as out_file:\n",
    "                        for filing in batch:\n",
    "                            out_file.write((json.dumps(filing) + '\\n').encode())\n",
    "                    batch_num += 1\n",
    "                    batch = [{'url': entry_url, 'timestamp': entry_time, 'descriptor': desc, 'sentence': sent, 'text_hash': text_hash}]\n",
    "\n",
    "\n",
    "with gzip.open(os.path.join(out_dir, 'batch_%s.gz' % batch_num), 'w') as out_file:\n",
    "    for filing in batch:\n",
    "        out_file.write((json.dumps(filing) + '\\n').encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517215d1-0c7c-47fb-a0d4-9b6542e48793",
   "metadata": {},
   "source": [
    "# Sample C4 according to desc2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40da67f2-db0a-4bb4-828b-502f03cccd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc2count_sample = pd.read_csv(os.path.join(SUPPORTING_DATA_DIR, DESCRIPTOR_TO_COUNT_CSV))\n",
    "desc2count_sample = desc2count_sample.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ec7f6-42de-40cb-9787-70fa853a3d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "C4_SENTENCE_BATCHES_DIR = f\"{C4_SENTENCE_BATCHES_DIR}_{current_date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7b319b5-877f-446e-b7c1-6afc201a49bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns_tbp = []\n",
    "\n",
    "for path, dirs, fns in os.walk(os.path.join(SUPPORTING_DATA_DIR, C4_SENTENCE_BATCHES_DIR)):\n",
    "    for fn in fns:\n",
    "        if os.path.splitext(fn)[-1] == '.gz':\n",
    "            fns_tbp.append(os.path.join(path, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "204de047-a85e-4445-95dc-bc8580162b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e7ce9716e2426a9e3572b01962f348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_dfs = []\n",
    "for path in tqdm(fns_tbp):\n",
    "    with gzip.open(path, 'r') as out_file:\n",
    "        sents_df = pd.read_json(out_file, lines=True, orient='records')\n",
    "        list_dfs.append(sents_df)\n",
    "\n",
    "c4_sents_df = pd.concat(list_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b27431ef-93de-4a0a-90ab-772728325f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c4_sents_df['text_hash'] = c4_sents_df['sentence'].apply(lambda x : hashlib.md5(x.encode()).hexdigest())\n",
    "c4_sents_df['desc_len'] = c4_sents_df['descriptor'].str.len()\n",
    "c4_sents_df = c4_sents_df.sort_values(by='desc_len', ascending = False).groupby('text_hash').head(1) # assign sentence to longest descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c015ca43-2b7b-4326-a320-ed38bc222c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_desc = list(desc2count_sample['descriptor'])\n",
    "all_desc = sorted(all_desc, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed4e770d-b84a-4c93-8cae-fa6ffb83eeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e53e7f5d5f843d5ac45c044f84465eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_samples = []\n",
    "# increment random state\n",
    "rand_state =1 \n",
    "for d in tqdm(all_desc):\n",
    "    n_samples = desc2count_sample[desc2count_sample['descriptor'] == d]['num_unique_sentences'].item()\n",
    "    grouped = c4_sents_df.groupby('descriptor').get_group(d)\n",
    " \n",
    "    len_grouped = grouped.shape[0]\n",
    "    if len_grouped >= n_samples:\n",
    "        all_samples.append(grouped.sample(n=n_samples, random_state=rand_state))\n",
    "    else:\n",
    "        all_samples.append(grouped.sample(n=n_samples, replace=True, random_state=rand_state))\n",
    "    rand_state += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "784845ff-5d67-4749-9fb2-8d5ad6240cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_c4_df = pd.concat(all_samples).reset_index()\n",
    "sampled_c4_df = sampled_c4_df.drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c48cb46-6074-4b84-9267-878276e75a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(SUPPORTING_DATA_DIR, f'{C4_PERSPECTIVE_SENTENCE_BATCHES_DIR}_{current_date}'), exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eafc731c-c3a0-4084-a82c-16b1d3f21c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e686be824e643b684bffef181d82be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrows = 10_000\n",
    "batch_groups = sampled_c4_df.groupby(sampled_c4_df.index // nrows)\n",
    "batch_n = 0\n",
    "for name, group in tqdm(batch_groups):\n",
    "    group.to_json(path_or_buf = os.path.join(SUPPORTING_DATA_DIR, f'{C4_PERSPECTIVE_SENTENCE_BATCHES_DIR}_{current_date}', 'batch_%s.gz' % batch_n), orient='records', lines=True)\n",
    "    batch_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e7119a-9e71-473a-aac8-cb08467d3836",
   "metadata": {},
   "source": [
    "# C4 Perspective analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "655f1a50-e749-4a24-9086-60483c5a5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns_tbp = []\n",
    "for path, dirs, fns in os.walk(os.path.join(SUPPORTING_DATA_DIR, f'{C4_PERSPECTIVE_SENTENCE_BATCHES_DIR}_{current_date}')):\n",
    "    for fn in fns:\n",
    "        if os.path.splitext(fn)[-1] == '.gz':\n",
    "            fns_tbp.append(os.path.join(path, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7903db-183d-4e13-aafc-ac84e2d1262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(cleaned_text, max_char_length):\n",
    "    if len(cleaned_text) <= max_char_length:\n",
    "        return [cleaned_text]\n",
    "    lines = iter(cleaned_text.splitlines())\n",
    "    chunks, current = [], next(lines)\n",
    "    for l in lines:\n",
    "        if len(current) + len(l) > max_char_length:\n",
    "            if len(current) > max_char_length:\n",
    "                current_chunks = [current[i:i + max_char_length] for i in range(0, len(current), max_char_length)]\n",
    "                chunks+= current_chunks\n",
    "                current = l\n",
    "                continue\n",
    "            chunks.append(current)\n",
    "            current = l\n",
    "        else:\n",
    "            current += '\\n' + l\n",
    "\n",
    "    # chunk the last line\n",
    "    if len(current) < max_char_length:\n",
    "        chunks.append(current)\n",
    "    else:\n",
    "        chunks += [current[i:i + max_char_length] for i in range(0, len(current), max_char_length)]\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ac7dd1-f949-4811-a601-3d4ee7e560be",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char_length = 18_000 # perspective api takes at most 20KB\n",
    "def get_perspective_toxicity(text, client, max_char_length = max_char_length):\n",
    "    \"\"\"\n",
    "    parameters: text (str), client (perspective api client)\n",
    "    returns: dictionary mapping the chunk to associated toxicity score\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = chunking(text, max_char_length)\n",
    "    chunks2score = {}\n",
    "    \n",
    "    for c in chunks:\n",
    "        if c.strip():\n",
    "            start = time.time()\n",
    "            analyze_request = {\n",
    "                'comment': {'text': c.strip()},\n",
    "                'languages': ['en'],\n",
    "                'requestedAttributes': {'TOXICITY': {}}\n",
    "            }\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "            end = time.time()\n",
    "            sleep_time = 1-(end-start)\n",
    "            time.sleep(max(sleep_time, 0))\n",
    "            chunks2score[c.strip()] = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "    return chunks2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5001d801-f17b-4040-a6bb-84b636865009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def work(sentence):\n",
    "    # build client here\n",
    "    client = discovery.build(\n",
    "        \"commentanalyzer\",\n",
    "        \"v1alpha1\",\n",
    "        developerKey=PERSPECTIVE_API_KEY,\n",
    "        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "        static_discovery=False,\n",
    "        )\n",
    "\n",
    "    sentence_hash = hashlib.md5(sentence.encode()).hexdigest()\n",
    "    tox_score = list(get_perspective_toxicity(sentence, client).values())[0]\n",
    "    return {sentence_hash:tox_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf1a550-629c-4f4a-b500-b8e825392050",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(SUPPORTING_DATA_DIR, f'{C4_PERSPECTIVE_SCORES_DIR}_{current_date}')\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "046f1a7a-8171-486b-9bf7-3f438873efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_toxicity_scores(batch_paths, out_dir = out_dir):\n",
    "    for batch in tqdm(batch_paths):\n",
    "        res_path = os.path.join(out_dir, batch.rsplit('/', 1)[1].split('.')[0] + '.pkl')\n",
    "\n",
    "        if os.path.exists(res_path):\n",
    "            with open(res_path, 'rb') as res_file:\n",
    "                res_dict = pickle.load(res_file)\n",
    "        else:\n",
    "            res_dict = dict()\n",
    "    \n",
    "        all_sent2hash= dict()\n",
    "\n",
    "        with gzip.open(batch, 'r') as batch_n:\n",
    "            for sent_info in batch_n.readlines():\n",
    "                info = json.loads(sent_info)\n",
    "                sent = info['sentence']\n",
    "                if len(sent.encode(\"utf8\")) > 20_000:\n",
    "                    continue\n",
    "                sent_hash = info['text_hash']\n",
    "                all_sent2hash[sent_hash] = sent\n",
    "    \n",
    "        to_process = [sent for hash, sent in all_sent2hash.items() if hash not in res_dict]\n",
    "        if len(to_process) == 0:\n",
    "            continue\n",
    "\n",
    "        n_workers = PERSPECTIVE_API_QPS-1\n",
    "        try:\n",
    "            with mp.Pool(n_workers) as p:\n",
    "                results = [r for r in tqdm(p.imap_unordered(work, to_process), total=len(to_process), miniters=1)]\n",
    "        except:\n",
    "            n_workers = n_workers//2\n",
    "            with mp.Pool(n_workers) as p:\n",
    "                results = [r for r in tqdm(p.imap_unordered(work, to_process), total=len(to_process), miniters=1)]\n",
    "       \n",
    "        for r in results:\n",
    "            if isinstance(r, dict):\n",
    "                res_dict.update(r)\n",
    "    \n",
    "        with open(res_path, 'wb') as out_file:\n",
    "            pickle.dump(res_dict, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f63b4a7-69da-4ec0-8f9a-76cc0baaa64f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5e12e1004c43d29361c901fec16105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_toxicity_scores(fns_tbp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
